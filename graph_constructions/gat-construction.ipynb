{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6b662f71-8455-49e0-bdc6-77ab146a63a4",
   "metadata": {},
   "source": [
    "## Loan Graph Construction\n",
    "\n",
    "This notebook constructs PyTorch Geometric graph objects from cleaned loan origination data (2015–2016), linking borrowers by shared geographic area, loan provider, or both. Nodes represent individual loans with engineered features, while edges encode relational metadata and group-size weighting. The resulting graphs—used for training, testing, and explanation—support GAT-based credit risk explainability pipelines by capturing borrower interconnectivity through structured edge types and temporal filtering.\n",
    "\n",
    "Note: This script is intended for academic reference only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3285a90e-4319-439e-9379-ac0b6c54303c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from torch_geometric.data import Data\n",
    "\n",
    "from tqdm.auto import tqdm \n",
    "\n",
    "import os\n",
    "\n",
    "DATA_ROOT_DIR = '../data'\n",
    "CLEANED_DATA_DIR = os.path.join(DATA_ROOT_DIR, 'cleaned_data')\n",
    "OUTPUT_GRAPH_DIR = os.path.join(DATA_ROOT_DIR, 'graph_data')\n",
    "GRAPH_D_TIMER_THRESHOLD = 1\n",
    "EDGE_BATCH_SIZE = 50 \n",
    "\n",
    "os.makedirs(OUTPUT_GRAPH_DIR, exist_ok=True)\n",
    "\n",
    "device = torch.device(\"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "EDGE_TYPE_AREA = 0\n",
    "EDGE_TYPE_PROVIDER = 1\n",
    "EDGE_TYPE_AREA_PROVIDER = 2\n",
    "NUM_EDGE_TYPES = 3 \n",
    "TOTAL_EDGE_DIM = NUM_EDGE_TYPES + 1\n",
    "\n",
    "def create_edges(group_indices_list, edge_type_id, num_edge_types_one_hot, debug_prefix=\"\", max_edges_per_group=20):\n",
    "\n",
    "    all_edges_tensors = []\n",
    "    all_edge_type_and_feature_tensors = []\n",
    "    total_groups_processed = 0\n",
    "    total_edges_added = 0\n",
    "\n",
    "    for i in tqdm(range(0, len(group_indices_list), EDGE_BATCH_SIZE), desc=f\"{debug_prefix} Sampling edge batches\"):\n",
    "        batch = group_indices_list[i:i+EDGE_BATCH_SIZE]\n",
    "\n",
    "        for indices in batch:\n",
    "            if len(indices) <= 1:\n",
    "                continue\n",
    "\n",
    "            idx_tensor = torch.tensor(indices, dtype=torch.long, device=device)\n",
    "            comb = torch.combinations(idx_tensor, r=2)\n",
    "\n",
    "            if comb.size(0) > max_edges_per_group:\n",
    "                perm = torch.randperm(comb.size(0), device=device)[:max_edges_per_group]\n",
    "                sampled = comb[perm]\n",
    "            else:\n",
    "                sampled = comb\n",
    "\n",
    "            bidir = torch.cat([\n",
    "                sampled,\n",
    "                sampled.flip(dims=[1])\n",
    "            ], dim=0).T\n",
    "\n",
    "            edge_attr_one_hot = torch.zeros((bidir.shape[1], num_edge_types_one_hot), dtype=torch.float32, device=device)\n",
    "            edge_attr_one_hot[:, edge_type_id] = 1.0\n",
    "            group_size_feat_value = 1.0 / len(indices)\n",
    "            extra_feat = torch.full((bidir.shape[1], 1), group_size_feat_value, device=device)\n",
    "            edge_attr_combined = torch.cat([edge_attr_one_hot, extra_feat], dim=1)\n",
    "            all_edges_tensors.append(bidir)\n",
    "            all_edge_type_and_feature_tensors.append(edge_attr_combined)\n",
    "            total_groups_processed += 1\n",
    "            total_edges_added += bidir.shape[1]\n",
    "\n",
    "    if all_edges_tensors:\n",
    "        edge_index = torch.cat(all_edges_tensors, dim=1)\n",
    "        edge_attr = torch.cat(all_edge_type_and_feature_tensors, dim=0)\n",
    "    else:\n",
    "        edge_index = torch.empty((2, 0), dtype=torch.long, device=device)\n",
    "        edge_attr = torch.empty((0, TOTAL_EDGE_DIM), dtype=torch.float32, device=device)\n",
    "\n",
    "    print(f\"{debug_prefix} Processed groups: {total_groups_processed:,}\")\n",
    "    print(f\"{debug_prefix} Total edges added: {total_edges_added:,}\")\n",
    "    return edge_index, edge_attr\n",
    "\n",
    "def create_loan_graph(origination_df, d_timer_threshold, graph_type):\n",
    "\n",
    "    print(f\"\\n### Constructing {graph_type} graph (d_timer >= {d_timer_threshold}) ###\")\n",
    "\n",
    "    if 'd_timer' in origination_df.columns:\n",
    "        origination_df['d_timer'] = pd.to_numeric(origination_df['d_timer'], errors='coerce')\n",
    "        origination_df['d_timer'] = origination_df['d_timer'].fillna(origination_df['d_timer'].median() if not origination_df['d_timer'].isnull().all() else 0)\n",
    "        print(f\"- d_timer min: {origination_df['d_timer'].min():.2f}, max: {origination_df['d_timer'].max():.2f}\")\n",
    "    else:\n",
    "        print(\"column not found in DataFrame. Cannot filter by d_timer.\")\n",
    "        origination_df['d_timer'] = d_timer_threshold + 1\n",
    "\n",
    "    filtered_df = origination_df[origination_df['d_timer'] >= d_timer_threshold].copy()\n",
    "    print(f\"Nodes after d_timer filtering: {len(filtered_df):,}\")\n",
    "\n",
    "    if filtered_df.empty:\n",
    "        print(f\"No nodes found for {graph_type} graph with d_timer >= {d_timer_threshold}\")\n",
    "        return None\n",
    "\n",
    "    final_nodes = filtered_df.reset_index(drop=True)\n",
    "    final_nodes['node_idx'] = final_nodes.index\n",
    "\n",
    "    feature_cols = ['fico','if_fthb','cnt_borr','cnt_units','dti','ltv',\n",
    "                    'orig_upb','loan_term','int_rt','if_prim_res','if_corr','if_sf','if_purc']\n",
    "    if 'current_upb' in final_nodes.columns:\n",
    "        feature_cols.append('current_upb')\n",
    "\n",
    "    existing_features = [f for f in feature_cols if f in final_nodes.columns]\n",
    "\n",
    "    for col in existing_features:\n",
    "        final_nodes[col] = pd.to_numeric(final_nodes[col], errors='coerce').fillna(0)\n",
    "\n",
    "    x = torch.tensor(final_nodes[existing_features].values, dtype=torch.float, device=device)\n",
    "    print(f\"Node features shape: {x.shape} | {x.device}\")\n",
    "\n",
    "    print(\"Preparing edge groups...\")\n",
    "    if 'area' not in final_nodes.columns or 'provider' not in final_nodes.columns:\n",
    "        print(\"'area' or 'provider' column missing. Cannot create grouping edges. Proceeding with no edges.\")\n",
    "        area_groups = []\n",
    "        provider_groups = []\n",
    "        area_provider_groups = []\n",
    "    else:\n",
    "        area_groups = [g['node_idx'].tolist() for _,g in final_nodes.groupby('area')]\n",
    "        provider_groups = [g['node_idx'].tolist() for _,g in final_nodes.groupby('provider')]\n",
    "        area_provider_groups = [g['node_idx'].tolist() for _,g in final_nodes.groupby(['area','provider'])]\n",
    "\n",
    "    print(\"Building edges...\")\n",
    "\n",
    "    print(\"\\n### Creating AREA edges ###\")\n",
    "    area_edges, area_edge_types = create_edges(\n",
    "        area_groups, EDGE_TYPE_AREA, NUM_EDGE_TYPES, debug_prefix=\"[AREA]\", max_edges_per_group=20\n",
    "    )\n",
    "    print(f\"Area edges created: {area_edges.shape[1]:,}\")\n",
    "\n",
    "    print(\"\\n### Creating PROVIDER edges ###\")\n",
    "    provider_edges, provider_edge_types = create_edges(\n",
    "        provider_groups, EDGE_TYPE_PROVIDER, NUM_EDGE_TYPES, debug_prefix=\"[PROVIDER]\", max_edges_per_group=20\n",
    "    )\n",
    "    print(f\"Provider edges created: {provider_edges.shape[1]:,}\")\n",
    "\n",
    "    print(\"\\n### Creating AREA+PROVIDER edges ###\")\n",
    "    area_provider_edges, area_provider_edge_types = create_edges(\n",
    "        area_provider_groups, EDGE_TYPE_AREA_PROVIDER, NUM_EDGE_TYPES, debug_prefix=\"[A+P]\", max_edges_per_group=20\n",
    "    )\n",
    "    print(f\"Area+Provider edges created: {area_provider_edges.shape[1]:,}\")\n",
    "\n",
    "    edges_to_concat = []\n",
    "    if area_edges.shape[1] > 0:\n",
    "        edges_to_concat.append(torch.cat([area_edges, area_edge_types.T], dim=0))\n",
    "    if provider_edges.shape[1] > 0:\n",
    "        edges_to_concat.append(torch.cat([provider_edges, provider_edge_types.T], dim=0))\n",
    "    if area_provider_edges.shape[1] > 0:\n",
    "        edges_to_concat.append(torch.cat([area_provider_edges, area_provider_edge_types.T], dim=0))\n",
    "\n",
    "    if not edges_to_concat:\n",
    "        print(\"No edges created across all types. Returning graph with no edges.\")\n",
    "        edge_index = torch.empty((2,0), dtype=torch.long, device=device)\n",
    "        edge_attr = torch.empty((0, TOTAL_EDGE_DIM), dtype=torch.float32, device=device)\n",
    "    else:\n",
    "        all_combined_edges_pre_dedup = torch.cat(edges_to_concat, dim=1).T\n",
    "        print(f\"Total edges (before unique_with_type): {all_combined_edges_pre_dedup.shape[0]:,}\")\n",
    "\n",
    "        unique_combined_edges = torch.unique(all_combined_edges_pre_dedup, dim=0)\n",
    "        edge_index = unique_combined_edges[:, :2].T\n",
    "        edge_attr = unique_combined_edges[:, 2:]\n",
    "\n",
    "        print(f\"Final unique edges (with type and feature distinction): {edge_index.shape[1]:,}\")\n",
    "\n",
    "    if 'default' in final_nodes.columns:\n",
    "        y = torch.tensor(final_nodes['default'].values, dtype=torch.long, device=device)\n",
    "    else:\n",
    "        print(\"'default' column not found for target. Using dummy target of zeros.\")\n",
    "        y = torch.zeros(len(final_nodes), dtype=torch.long, device=device)\n",
    "\n",
    "    graph_data = Data(x=x, edge_index=edge_index, edge_attr=edge_attr, y=y)\n",
    "    graph_data.num_nodes = len(final_nodes)\n",
    "\n",
    "    print(f\"Graph constructed | Nodes: {graph_data.num_nodes:,} | Edges: {graph_data.edge_index.shape[1]:,}\")\n",
    "    print(f\"Edge attributes shape: {graph_data.edge_attr.shape} (Expected: (num_edges, {TOTAL_EDGE_DIM}))\")\n",
    "\n",
    "    return graph_data\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"\\nLoading data for graph construction...\")\n",
    "\n",
    "    train_graph_path = '../data/cleaned_data/df_origination_train_graph_scaled.csv'\n",
    "    test_graph_path = '../data/cleaned_data/df_origination_test_graph_scaled.csv'\n",
    "\n",
    "    try:\n",
    "        df_train_graph = pd.read_csv(train_graph_path)\n",
    "        df_test_graph = pd.read_csv(test_graph_path)\n",
    "        print(f\"Train graph data: {df_train_graph.shape} | Test graph data: {df_test_graph.shape}\")\n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"Error loading graph data CSVs.\")\n",
    "        print(f\"Details: {e}\")\n",
    "        exit() \n",
    "\n",
    "    print(\"\\nBuilding training graph (2015 Jan-Jun)...\")\n",
    "    train_graph = create_loan_graph(df_train_graph, GRAPH_D_TIMER_THRESHOLD, 'train')\n",
    "    if train_graph:\n",
    "        torch.save(train_graph, os.path.join(OUTPUT_GRAPH_DIR, 'train_graph.pt'))\n",
    "        print(f\"Saved training graph to {os.path.join(OUTPUT_GRAPH_DIR, 'train_graph.pt')}\")\n",
    "    else:\n",
    "        print(\"Training graph was not built (likely due to no nodes).\")\n",
    "\n",
    "    print(\"\\nBuilding standard test graph (2016 Jan-Jun)...\")\n",
    "    test_graph = create_loan_graph(df_test_graph, GRAPH_D_TIMER_THRESHOLD, 'test')\n",
    "    if test_graph:\n",
    "        torch.save(test_graph, os.path.join(OUTPUT_GRAPH_DIR, 'test_graph.pt'))\n",
    "        print(f\"Saved standard test graph to {os.path.join(OUTPUT_GRAPH_DIR, 'test_graph.pt')}\")\n",
    "    else:\n",
    "        print(\"Standard test graph was not built.\")\n",
    "\n",
    "    print(\"\\nLoading data for July explanation graph construction...\")\n",
    "\n",
    "    explanation_data_path = '../data/cleaned_data/df_origination_test_explanation_scaled.csv'\n",
    "    try:\n",
    "        df_explanation_raw = pd.read_csv(explanation_data_path)\n",
    "        print(f\"Raw explanation data loaded: {df_explanation_raw.shape}\")\n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"Error loading explanation data CSV.\")\n",
    "        print(f\"Details: {e}\")\n",
    "        df_explanation_raw = pd.DataFrame()\n",
    "\n",
    "    if 'month' in df_explanation_raw.columns and 'year' in df_explanation_raw.columns:\n",
    "        df_explanation_raw['month'] = pd.to_numeric(df_explanation_raw['month'], errors='coerce')\n",
    "        df_explanation_raw['year'] = pd.to_numeric(df_explanation_raw['year'], errors='coerce')\n",
    "        df_july_originations = df_explanation_raw[\n",
    "            (df_explanation_raw['year'] == 2016) &\n",
    "            (df_explanation_raw['month'] == 7)\n",
    "        ].copy()\n",
    "\n",
    "        print(f\"Filtered for 2016 July originations: {df_july_originations.shape}\")\n",
    "\n",
    "        if df_july_originations.empty:\n",
    "            print(\"No 2016 July originated loans found in explanation dataset. Cannot build July explanation graph.\")\n",
    "            july_explanation_graph = None\n",
    "        else:\n",
    "            print(\"\\nBuilding July explanation graph (2016 July Originations)...\")\n",
    "            july_explanation_graph = create_loan_graph(df_july_originations,\n",
    "                                                           d_timer_threshold=0,\n",
    "                                                           graph_type='july_explanation')\n",
    "            if july_explanation_graph:\n",
    "                torch.save(july_explanation_graph, os.path.join(OUTPUT_GRAPH_DIR, 'july_explanation_graph.pt'))\n",
    "                print(f\"Saved July explanation graph to {os.path.join(OUTPUT_GRAPH_DIR, 'july_explanation_graph.pt')}\")\n",
    "            else:\n",
    "                print(\"July explanation graph was not built (likely due to no nodes).\")\n",
    "    else:\n",
    "        print(\"'month' or 'year' columns not found in df_origination_test_explanation_scaled.csv. Cannot filter for July originations.\")\n",
    "\n",
    "    print(\"\\nDone! All graphs saved to:\", OUTPUT_GRAPH_DIR)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (graphenv)",
   "language": "python",
   "name": "graphenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
