{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "872e748f-a0bf-449d-8861-38bb2e61a912",
   "metadata": {},
   "source": [
    "# Preparing the 2015 and 2016 Freddie Mac Dataset\n",
    "\n",
    "This notebook transforms raw mortgage loan data from 2015 and 2016 into a structured format suitable for machine learning. It performs archive extraction, data validation, placeholder cleaning, default outcome labeling, temporal filtering, and balanced sampling. The result is a cleaned and labeled dataset that enables robust downstream modeling for credit risk prediction.\n",
    "\n",
    "Note: This script is intended for academic reference only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cebdb30-a155-4d8d-bdd1-92b4605ccb3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from datetime import datetime\n",
    "from dateutil.relativedelta import relativedelta\n",
    "\n",
    "import zipfile\n",
    "\n",
    "import os\n",
    "\n",
    "def extract_zip_files(zip_dir, extract_to_dir):\n",
    "    \n",
    "    if not os.path.exists(extract_to_dir):\n",
    "        os.makedirs(extract_to_dir)\n",
    "        print(f\"Created extraction directory: {extract_to_dir}\")\n",
    "\n",
    "    for item in os.listdir(zip_dir):\n",
    "        full_path = os.path.join(zip_dir, item)\n",
    "        if item.endswith(\".zip\"):\n",
    "            try:\n",
    "                with zipfile.ZipFile(full_path, 'r') as zip_ref:\n",
    "                    zip_ref.extractall(extract_to_dir)\n",
    "                print(f\"Successfully extracted {item} to {extract_to_dir}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error: An unexpected error occurred.\")\n",
    "        else:\n",
    "            print(f\"Skipping non-zip file: {item}\")\n",
    "\n",
    "def load_freddie_mac_data(year_quarter_str, data_type, base_path):\n",
    "    \n",
    "    file_prefix = 'origdata' if data_type == 'origination' else 'svcgdata'\n",
    "    file_name = f\"{file_prefix} {year_quarter_str}.csv\"\n",
    "    full_file_path = os.path.join(base_path, file_name)\n",
    "\n",
    "    if not os.path.exists(full_file_path):\n",
    "        print(f\"File not found: {full_file_path}. Skipping.\")\n",
    "        return None\n",
    "    try:\n",
    "        df = pd.read_csv(full_file_path, low_memory=False)\n",
    "        print(f\"Successfully loaded {data_type} data from {file_name}.\")\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        print(f\"Error: Error loading {data_type} data from {full_file_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "def handle_freddie_mac_placeholders(data_df, columns_to_handle):\n",
    "\n",
    "    processed_data = data_df.copy()\n",
    "    \n",
    "    placeholder_map = {\n",
    "        'fico': 9999,\n",
    "        'flag_fthb': '9',\n",
    "        'mi_pct': 999,\n",
    "        'cnt_units': 99,\n",
    "        'occpy_sts': '9',\n",
    "        'dti': 999,\n",
    "        'ltv': 999,\n",
    "        'channel': '9',\n",
    "        'prop_type': '99',\n",
    "        'loan_purpose': '9',\n",
    "        'cnt_borr': 99\n",
    "    }\n",
    "\n",
    "    for col in columns_to_handle:\n",
    "        if col in processed_data.columns:\n",
    "            current_dtype = processed_data[col].dtype\n",
    "            \n",
    "            if pd.api.types.is_numeric_dtype(current_dtype):\n",
    "                processed_data[col] = pd.to_numeric(processed_data[col], errors='coerce')\n",
    "                placeholder = placeholder_map.get(col, np.nan)\n",
    "                processed_data[col] = processed_data[col].replace(placeholder, np.nan)\n",
    "            else:\n",
    "                processed_data[col] = processed_data[col].astype(str)\n",
    "                placeholder = str(placeholder_map.get(col, ''))\n",
    "                processed_data[col] = processed_data[col].replace(placeholder, np.nan)\n",
    "            \n",
    "            print(f\"Replaced placeholder in column '{col}'\")\n",
    "\n",
    "    if 'flag_sc' in processed_data.columns and 'flag_sc' in columns_to_handle:\n",
    "        processed_data['flag_sc'] = processed_data['flag_sc'].fillna('N').astype(str)\n",
    "    \n",
    "    if 'flag_fthb' in processed_data.columns and 'flag_fthb' in columns_to_handle:\n",
    "        processed_data['flag_fthb'] = processed_data['flag_fthb'].fillna('N').astype(str)\n",
    "    \n",
    "    if 'occpy_sts' in processed_data.columns and 'occpy_sts' in columns_to_handle:\n",
    "        processed_data['occpy_sts'] = processed_data['occpy_sts'].fillna('U').astype(str)\n",
    "        \n",
    "    return processed_data\n",
    "\n",
    "def calculate_d_timer(svcg_cycle_str, reference_date_str='201502'):\n",
    "\n",
    "    if pd.isna(svcg_cycle_str) or svcg_cycle_str == 'N_A':\n",
    "        return 1000\n",
    "    \n",
    "    svcg_cycle_str = str(svcg_cycle_str)\n",
    "    \n",
    "    if not svcg_cycle_str.isdigit() or len(svcg_cycle_str) != 6:\n",
    "        return 1000\n",
    "    \n",
    "    try:\n",
    "        current_date = datetime.strptime(svcg_cycle_str, \"%Y%m\")\n",
    "        base_date = datetime.strptime(reference_date_str, \"%Y%m\")\n",
    "        r = relativedelta(current_date, base_date)\n",
    "        return r.months + (12 * r.years)\n",
    "    except ValueError:\n",
    "        return 1000\n",
    "\n",
    "def process_performance_for_defaults(performance_df, observation_cutoff_date):\n",
    "    \n",
    "    if performance_df is None or performance_df.empty:\n",
    "        return pd.DataFrame({'id_loan': [], 'loan_defaulted': [], 'svcg_cycle': []})\n",
    "\n",
    "    performance_df = performance_df.copy()\n",
    "    performance_df['id_loan'] = performance_df['id_loan'].astype(str)\n",
    "    \n",
    "    if 'svcg_cycle' in performance_df.columns:\n",
    "        performance_df['svcg_cycle'] = performance_df['svcg_cycle'].astype(str)\n",
    "        performance_df['svcg_cycle'] = performance_df['svcg_cycle'].apply(\n",
    "            lambda x: x if len(str(x)) == 6 and str(x).isdigit() else 'N_A'\n",
    "        )\n",
    "    else:\n",
    "        performance_df['svcg_cycle'] = 'N_A'\n",
    "    \n",
    "    print(f\"Filtering performance data to only include data up to {observation_cutoff_date}\")\n",
    "    \n",
    "    valid_cycles = performance_df['svcg_cycle'] != 'N_A'\n",
    "    performance_df_filtered = performance_df[valid_cycles].copy()\n",
    "    cutoff_filter = performance_df_filtered['svcg_cycle'] <= observation_cutoff_date\n",
    "    performance_df_filtered = performance_df_filtered[cutoff_filter].copy()\n",
    "    \n",
    "    print(f\"After filtering: {len(performance_df_filtered)} performance records (from {len(performance_df)} original)\")\n",
    "    \n",
    "    if performance_df_filtered.empty:\n",
    "        print(\"Warning: No performance data available within observation period\")\n",
    "        return pd.DataFrame({'id_loan': [], 'loan_defaulted': [], 'svcg_cycle': []})\n",
    "    \n",
    "    zero_bal_defaults = pd.DataFrame()\n",
    "    if 'cd_zero_bal' in performance_df_filtered.columns:\n",
    "        zero_bal_defaults = performance_df_filtered[\n",
    "            (performance_df_filtered['cd_zero_bal'] == 3) | \n",
    "            (performance_df_filtered['cd_zero_bal'] == 6) | \n",
    "            (performance_df_filtered['cd_zero_bal'] == 9)\n",
    "        ]\n",
    "    \n",
    "    performance_df_filtered.loc[:, 'delq_sts_numeric'] = performance_df_filtered['delq_sts'].replace('R', '-1', regex=False)\n",
    "    performance_df_filtered.loc[:, 'delq_sts_numeric'] = pd.to_numeric(performance_df_filtered['delq_sts_numeric'], errors='coerce')\n",
    "    initial_rows = len(performance_df_filtered)\n",
    "    performance_df_filtered = performance_df_filtered.dropna(subset=['delq_sts_numeric']).reset_index(drop=True)\n",
    "    \n",
    "    if len(performance_df_filtered) < initial_rows:\n",
    "        print(f\"Removed {initial_rows - len(performance_df_filtered)} rows with invalid 'delq_sts'.\")\n",
    "    \n",
    "    latest_performance = performance_df_filtered.sort_values('svcg_cycle', ascending=False).drop_duplicates('id_loan', keep='first')\n",
    "    \n",
    "    delq_defaults = performance_df_filtered[\n",
    "        (performance_df_filtered['delq_sts_numeric'] == -1) | \n",
    "        (performance_df_filtered['delq_sts_numeric'] >= 3)\n",
    "    ]\n",
    "    \n",
    "    all_defaults = pd.concat([delq_defaults, zero_bal_defaults]).drop_duplicates('id_loan', keep='first')\n",
    "    defaults_flag = pd.DataFrame({'id_loan': all_defaults['id_loan'].unique(), 'loan_defaulted': 1})\n",
    "    \n",
    "    defaults_flag = pd.merge(\n",
    "        defaults_flag, \n",
    "        latest_performance[['id_loan', 'svcg_cycle']], \n",
    "        on='id_loan', \n",
    "        how='outer'\n",
    "    )\n",
    "    \n",
    "    print(f\"Identified {len(defaults_flag)} defaulted loans within observation period (up to {observation_cutoff_date})\")\n",
    "    return defaults_flag\n",
    "\n",
    "def filter_loans_with_sufficient_history(merged_data, min_months=12):\n",
    "    \n",
    "    print(\"Filtering loans with sufficient history.\")\n",
    "    \n",
    "    if 'd_timer' not in merged_data.columns:\n",
    "        print(\"Warning: 'd_timer' column not found, cannot filter by loan history\")\n",
    "        return merged_data\n",
    "    \n",
    "    sufficient_history = merged_data[merged_data['d_timer'] >= min_months].copy()\n",
    "    print(f\"Filtered from {len(merged_data)} to {len(sufficient_history)} loans with â‰¥{min_months} months OBSERVED history\")\n",
    "    \n",
    "    return sufficient_history\n",
    "\n",
    "def filter_for_complete_time_series(performance_df, merged_data, observation_cutoff_date, min_months_observed=6):\n",
    "\n",
    "    print(\"Filtering for loans with sufficient observed time series.\")\n",
    "    \n",
    "    if performance_df is None or performance_df.empty:\n",
    "        print(\"Performance data is empty, cannot filter for time series completeness\")\n",
    "        return merged_data\n",
    "    \n",
    "    performance_df = performance_df.copy()\n",
    "    performance_df['svcg_cycle'] = performance_df['svcg_cycle'].astype(str)\n",
    "    valid_cycles = (performance_df['svcg_cycle'] != 'N_A') & (performance_df['svcg_cycle'] <= observation_cutoff_date)\n",
    "    performance_filtered = performance_df[valid_cycles].copy()\n",
    "    loan_observation_counts = performance_filtered.groupby('id_loan').size()\n",
    "    sufficient_loans = loan_observation_counts[loan_observation_counts >= min_months_observed].index\n",
    "    complete_loans_data = merged_data[merged_data['id_loan'].isin(sufficient_loans)].copy()\n",
    "    \n",
    "    print(f\"Filtered from {len(merged_data)} to {len(complete_loans_data)} loans with â‰¥{min_months_observed} observed months\")\n",
    "    return complete_loans_data\n",
    "\n",
    "def balanced_downsampling(merged_data, target_sample_size=40000):\n",
    "\n",
    "    print(\"Applying balanced downsampling...\")\n",
    "    \n",
    "    if len(merged_data) <= target_sample_size:\n",
    "        print(f\"Data size ({len(merged_data)}) is already smaller than target size ({target_sample_size}). No downsampling needed.\")\n",
    "        return merged_data\n",
    "    \n",
    "    target_default_rate = 0.06\n",
    "    default_col = 'loan_defaulted'\n",
    "    defaults_df = merged_data[merged_data[default_col] == 1]\n",
    "    non_defaults_df = merged_data[merged_data[default_col] == 0]\n",
    "    num_defaults = int(target_sample_size * target_default_rate)\n",
    "    num_non_defaults = target_sample_size - num_defaults\n",
    "    \n",
    "    if len(defaults_df) < num_defaults:\n",
    "        print(f\"Warning: Only {len(defaults_df)} defaulted loans available, using all of them.\")\n",
    "        num_defaults = len(defaults_df)\n",
    "        num_non_defaults = target_sample_size - num_defaults\n",
    "\n",
    "        if target_sample_size > 0:\n",
    "            actual_achieved_rate = num_defaults / target_sample_size\n",
    "            print(f\"Actual achieved default rate due to limited defaults: {actual_achieved_rate*100:.2f}%\")\n",
    "        else:\n",
    "            actual_achieved_rate = 0\n",
    "    else:\n",
    "        actual_achieved_rate = target_default_rate\n",
    "\n",
    "    if len(non_defaults_df) < num_non_defaults:\n",
    "        print(f\"Warning: Only {len(non_defaults_df)} non-defaulted loans available, using all of them.\")\n",
    "        num_non_defaults = len(non_defaults_df)\n",
    "        num_defaults = target_sample_size - num_non_defaults\n",
    "        if target_sample_size > 0:\n",
    "            actual_achieved_rate = num_defaults / target_sample_size\n",
    "            print(f\"Actual achieved default rate due to limited non-defaults: {actual_achieved_rate*100:.2f}%\")\n",
    "        else:\n",
    "            actual_achieved_rate = 0\n",
    "\n",
    "    sampled_defaults = defaults_df.sample(n=num_defaults, random_state=42)\n",
    "    sampled_non_defaults = non_defaults_df.sample(n=num_non_defaults, random_state=42)\n",
    "    downsampled_data = pd.concat([sampled_defaults, sampled_non_defaults]).sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "    \n",
    "    print(f\"Downsampled to {len(downsampled_data)} samples with {actual_achieved_rate*100:.2f}% defaults ({len(sampled_defaults)} defaulted loans).\")\n",
    "    return downsampled_data\n",
    "\n",
    "def process_freddie_mac_quarterly_data(year, quarter, extracted_csv_dir, observation_cutoff_date, target_sample_size=40000):\n",
    "\n",
    "    print(f\"Starting data processing for {year}-{quarter}.\")\n",
    "    print(f\"Observation cutoff date: {observation_cutoff_date}\")\n",
    "    \n",
    "    current_year_extracted_dir = os.path.join(extracted_csv_dir, str(year))\n",
    "\n",
    "    year_quarter_str = f\"{year}-{quarter}\"\n",
    "    origination_df = load_freddie_mac_data(year_quarter_str, 'origination', current_year_extracted_dir)\n",
    "    performance_df = load_freddie_mac_data(year_quarter_str, 'performance', current_year_extracted_dir)\n",
    "\n",
    "    if origination_df is None or performance_df is None:\n",
    "        print(f\"Error: Failed to load essential data for {year_quarter_str}. Skipping further processing for this quarter.\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    origination_df.loc[:, 'id_loan'] = origination_df['id_loan'].astype(str)\n",
    "    performance_df.loc[:, 'id_loan'] = performance_df['id_loan'].astype(str)\n",
    "    defaults_flag_df = process_performance_for_defaults(performance_df, observation_cutoff_date)\n",
    "    \n",
    "    orig_cols_needed = [\n",
    "        'id_loan', 'fico', 'flag_fthb', 'mi_pct', 'cnt_units', 'occpy_sts',\n",
    "        'dti', 'ltv', 'channel', 'prod_type', 'prop_type', 'loan_purpose',\n",
    "        'cnt_borr', 'seller_name', 'flag_sc', 'servicer_name', 'orig_upb',\n",
    "        'dt_first_pi', 'orig_loan_term', 'zipcode', 'loan_age'\n",
    "    ]\n",
    "    \n",
    "    existing_orig_cols_needed = [col for col in orig_cols_needed if col in origination_df.columns]\n",
    "    origination_df_filtered = origination_df[existing_orig_cols_needed].copy()\n",
    "     \n",
    "    if 'dt_first_pi' in origination_df_filtered.columns:\n",
    "        origination_df_filtered.loc[:, 'dt_first_pi_str'] = origination_df_filtered['dt_first_pi'].astype(str)\n",
    "        initial_rows_before_date_filter = len(origination_df_filtered)\n",
    "        valid_date_rows = origination_df_filtered['dt_first_pi_str'].str.len() == 6\n",
    "        origination_df_filtered = origination_df_filtered[valid_date_rows].reset_index(drop=True)\n",
    "        \n",
    "        if len(origination_df_filtered) < initial_rows_before_date_filter:\n",
    "            print(f\"Dropped {initial_rows_before_date_filter - len(origination_df_filtered)} rows with invalid 'dt_first_pi' string length (not YYYYMM).\")\n",
    "\n",
    "        if not origination_df_filtered.empty:\n",
    "            origination_df_filtered.loc[:, 'year'] = origination_df_filtered['dt_first_pi_str'].str[0:4].astype(int)\n",
    "            origination_df_filtered.loc[:, 'month'] = origination_df_filtered['dt_first_pi_str'].str[4:6].astype(int)\n",
    "            \n",
    "            initial_rows_before_month_filter = len(origination_df_filtered)\n",
    "            valid_month_rows = (origination_df_filtered['month'] >= 1) & (origination_df_filtered['month'] <= 12)\n",
    "            origination_df_filtered = origination_df_filtered[valid_month_rows].reset_index(drop=True)\n",
    "            \n",
    "            if len(origination_df_filtered) < initial_rows_before_month_filter:\n",
    "                print(f\"Dropped {initial_rows_before_month_filter - len(origination_df_filtered)} rows with invalid month values in 'dt_first_pi'.\")\n",
    "\n",
    "            origination_df_filtered.drop(columns=['dt_first_pi_str'], inplace=True)\n",
    "    \n",
    "    orig_cols_for_placeholder_map = [\n",
    "        'fico', 'flag_fthb', 'mi_pct', 'cnt_units', 'occpy_sts', 'dti', 'ltv',\n",
    "        'channel', 'prop_type', 'loan_purpose', 'cnt_borr'\n",
    "    ]\n",
    "    \n",
    "    if 'flag_sc' in existing_orig_cols_needed:\n",
    "        orig_cols_for_placeholder_map.append('flag_sc')\n",
    "\n",
    "    actual_cols_for_placeholder = [col for col in orig_cols_for_placeholder_map if col in origination_df_filtered.columns]\n",
    "    origination_df_filtered = handle_freddie_mac_placeholders(origination_df_filtered, actual_cols_for_placeholder)\n",
    "    merged_data = pd.merge(origination_df_filtered, defaults_flag_df, on='id_loan', how='left')\n",
    "    merged_data.loc[:, 'loan_defaulted'] = merged_data['loan_defaulted'].fillna(0).astype(int)\n",
    "    merged_data.loc[:, 'svcg_cycle'] = merged_data['svcg_cycle'].fillna('N_A')\n",
    "    \n",
    "    print(\"Merged origination data with loan default flags and observed servicing cycle.\")\n",
    "    \n",
    "    if 'svcg_cycle' in merged_data.columns:\n",
    "        merged_data['d_timer'] = merged_data['svcg_cycle'].apply(\n",
    "            lambda x: calculate_d_timer(x, reference_date_str='201502')\n",
    "        )\n",
    "        print(\"Added 'd_timer' feature measuring months since February 2015.\")\n",
    "\n",
    "    merged_data = filter_loans_with_sufficient_history(merged_data, min_months=6)\n",
    "    merged_data = filter_for_complete_time_series(performance_df, merged_data, observation_cutoff_date, min_months_observed=6)\n",
    "    \n",
    "    if len(merged_data) > target_sample_size:\n",
    "        merged_data = balanced_downsampling(merged_data, target_sample_size)\n",
    "\n",
    "    if 'Unnamed: 0' in merged_data.columns:\n",
    "        merged_data = merged_data.drop(columns=['Unnamed: 0'])\n",
    "        \n",
    "    merged_data.loc[:, 'id'] = np.arange(1, len(merged_data) + 1)\n",
    "\n",
    "    desired_order_prefix = [\n",
    "        'id', 'id_loan', 'year', 'month', 'svcg_cycle', 'd_timer',\n",
    "        'fico', 'flag_fthb', 'mi_pct', 'cnt_units', 'occpy_sts', 'dti',\n",
    "        'ltv', 'channel', 'prop_type', 'loan_purpose', 'cnt_borr', 'flag_sc',\n",
    "        'orig_upb', 'loan_defaulted', 'orig_loan_term', 'seller_name', 'servicer_name', 'zipcode'\n",
    "    ]\n",
    "    final_cols = [col for col in desired_order_prefix if col in merged_data.columns]\n",
    "    for col in merged_data.columns:\n",
    "        if col not in final_cols:\n",
    "            final_cols.append(col)\n",
    "\n",
    "    merged_data = merged_data[final_cols].reset_index(drop=True)\n",
    "    print(f\"### Finished data processing for {year}-{quarter} ###\")\n",
    "    return merged_data\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    pd.set_option('future.no_silent_downcasting', True)\n",
    "    RAW_ZIP_DATA_DIR = '../data/raw_data'\n",
    "    EXTRACTED_CSV_DATA_DIR = '../data/extracted_data'\n",
    "    OUTPUT_PREPARED_DATA_DIR = '../data/prepared_data'\n",
    "    TARGET_SAMPLE_SIZE_PER_QUARTER = 15000\n",
    "\n",
    "    if not os.path.exists(OUTPUT_PREPARED_DATA_DIR):\n",
    "        os.makedirs(OUTPUT_PREPARED_DATA_DIR)\n",
    "        print(f\"Created output directory for prepared data: {OUTPUT_PREPARED_DATA_DIR}\")\n",
    "\n",
    "\n",
    "    OBSERVATION_CUTOFFS = {\n",
    "        2015: '201612', \n",
    "        2016: '201712' \n",
    "    }\n",
    "\n",
    "    years_to_process = [2015, 2016]\n",
    "    all_yearly_dataframes = {}\n",
    "\n",
    "    for current_target_year in years_to_process:\n",
    "        \n",
    "        observation_cutoff = OBSERVATION_CUTOFFS[current_target_year]\n",
    "        print(f\"\\nStarting overall data processing for {current_target_year}\")\n",
    "        print(f\"### Observation cutoff: {observation_cutoff}.###\")\n",
    "\n",
    "        current_year_zip_dir = os.path.join(RAW_ZIP_DATA_DIR, str(current_target_year))\n",
    "        current_year_extracted_dir = os.path.join(EXTRACTED_CSV_DATA_DIR, str(current_target_year))\n",
    "        print(f\"Extracting all zip files for {current_target_year} from {current_year_zip_dir} to {current_year_extracted_dir}...\")\n",
    "        extract_zip_files(current_year_zip_dir, current_year_extracted_dir)\n",
    "        print(f\"Finished initial extraction for {current_target_year}.\\n\")\n",
    "\n",
    "        all_quarters_data_for_year = []\n",
    "\n",
    "        for quarter_num in range(1, 5):\n",
    "            current_quarter_str = f'Q{quarter_num}'\n",
    "            print(f\"\\n### Processing {current_target_year}-{current_quarter_str} Data. ###\")\n",
    "\n",
    "            processed_quarter_data = process_freddie_mac_quarterly_data(\n",
    "                current_target_year, current_quarter_str, EXTRACTED_CSV_DATA_DIR, \n",
    "                observation_cutoff, TARGET_SAMPLE_SIZE_PER_QUARTER\n",
    "            )\n",
    "\n",
    "            if not processed_quarter_data.empty:\n",
    "                all_quarters_data_for_year.append(processed_quarter_data)\n",
    "            else:\n",
    "                print(f\"No data processed for {current_target_year}-{current_quarter_str}.\")\n",
    "\n",
    "        df_prepared_for_year = pd.DataFrame()\n",
    "        if all_quarters_data_for_year:\n",
    "            print(f\"Attempting to concatenate {len(all_quarters_data_for_year)} quarters of {current_target_year} data.\")\n",
    "            try:\n",
    "                df_prepared_for_year = pd.concat(all_quarters_data_for_year, ignore_index=True)\n",
    "                print(f\"Successfully concatenated all {current_target_year} quarters. Total rows: {len(df_prepared_for_year)}\")\n",
    "\n",
    "                all_yearly_dataframes[current_target_year] = df_prepared_for_year\n",
    "\n",
    "                output_filename = os.path.join(OUTPUT_PREPARED_DATA_DIR, f'prepared_{current_target_year}_full_year_data.csv')\n",
    "                df_prepared_for_year.to_csv(output_filename, index=False)\n",
    "                print(f\"Saved prepared {current_target_year} full year data to '{output_filename}'\")\n",
    "\n",
    "                print(f\"\\n### Full Year {current_target_year} Data: Final Combined Inspection ###\")\n",
    "                print(\"Shape:\", df_prepared_for_year.shape)\n",
    "                print(\"Missing values (top 5 columns):\\n\", df_prepared_for_year.isnull().sum().nlargest(5))\n",
    "                print(\"Default counts in full year data:\\n\", df_prepared_for_year['loan_defaulted'].value_counts(dropna=False))\n",
    "                print(\"Unique loan IDs in full year data:\", df_prepared_for_year['id_loan'].nunique())\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error concatenating {current_target_year} dataframes: {e}\")\n",
    "        else:\n",
    "            print(f\"Warning: No {current_target_year} dataframes were successfully processed for concatenation.\")\n",
    "\n",
    "    print(f\"\\n### Finished all data processing for years {years_to_process} ###\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (llmvenv)",
   "language": "python",
   "name": "llmvenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
