{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "45ce3d82-ed00-4ef4-955f-414cc4505e40",
   "metadata": {},
   "source": [
    "# Cleaning the Dataset\n",
    "\n",
    "This notebook continues from the initial data preparation process and performs systematic cleaning, feature engineering, and scaling to ready the dataset for use in predictive modeling pipelines. It includes standardization of categorical fields, correction of numeric anomalies, creation of derived features, and temporal splitting into training, testing, and explanation subsets. The cleaned outputs are optimized for graph- and tabular-based credit risk models.\n",
    "\n",
    "Note: This script is intended for academic reference only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe12502e-c282-49bc-9519-23e02be2ae9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "from datetime import datetime\n",
    "from dateutil.relativedelta import relativedelta\n",
    "\n",
    "import os\n",
    "\n",
    "def clean_and_engineer_features(dataframe: pd.DataFrame) -> pd.DataFrame:\n",
    "\n",
    "    print(\"\\nStarting cleaning and feature engineering\")\n",
    "    df = dataframe.copy()\n",
    "    print(f\"Initial DataFrame shape: {df.shape}\")\n",
    "\n",
    "    cols_to_drop = [\n",
    "        'dt_matr', 'cd_msa', 'cltv', 'st',\n",
    "        'servicer_name', 'prod_type', 'ppmt_pnlty'\n",
    "    ]\n",
    "    existing_cols_to_drop = [col for col in cols_to_drop if col in df.columns]\n",
    "    if existing_cols_to_drop:\n",
    "        df.drop(existing_cols_to_drop, axis=1, inplace=True)\n",
    "        print(f\"Dropped columns: {', '.join(existing_cols_to_drop)}\")\n",
    "\n",
    "    print(\"Creating 'dt_orig' from 'year' and 'month' for temporal splitting.\")\n",
    "    if 'year' in df.columns and 'month' in df.columns:\n",
    "        df['year'] = pd.to_numeric(df['year'], errors='coerce').fillna(0).astype(int)\n",
    "        df['month'] = pd.to_numeric(df['month'], errors='coerce').fillna(0).astype(int)\n",
    "\n",
    "        initial_rows = len(df)\n",
    "        df = df[(df['year'] >= 1900) & (df['year'] <= datetime.now().year + 1) &\n",
    "                (df['month'] >= 1) & (df['month'] <= 12)].copy()\n",
    "        if len(df) < initial_rows:\n",
    "            print(f\"Removed {initial_rows - len(df)} rows with invalid year/month combinations for 'dt_orig'.\")\n",
    "\n",
    "        df['dt_orig'] = df['year'].astype(str) + df['month'].astype(str).str.zfill(2)\n",
    "        print(f\"'dt_orig' created. Sample values: {df['dt_orig'].head().tolist()}\")\n",
    "    else:\n",
    "        print(\"'year' or 'month' columns not found. Cannot create 'dt_orig'.\")\n",
    "\n",
    "    if 'dt_first_pi' in df.columns:\n",
    "        df.drop(columns=['dt_first_pi'], inplace=True)\n",
    "        print(\"Dropped original 'dt_first_pi' column as 'dt_orig' is now created.\")\n",
    "\n",
    "    print(\"Processing categorical features to binary/renaming:\")\n",
    "    if 'flag_fthb' in df.columns:\n",
    "        print(f\"Original flag_fthb values: {df['flag_fthb'].unique()}\")\n",
    "        df['if_fthb'] = df['flag_fthb'].replace({'Y': 1, 'N': 0, 'Other': 0}).fillna(0).astype(int)\n",
    "        df.drop(columns=['flag_fthb'], inplace=True)\n",
    "        print(f\"'flag_fthb' -> 'if_fthb': {df['if_fthb'].value_counts().to_dict()}\")\n",
    "\n",
    "    if 'occpy_sts' in df.columns:\n",
    "        print(f\"Original occpy_sts values: {df['occpy_sts'].unique()}\")\n",
    "        df['if_prim_res'] = df['occpy_sts'].replace({'P': 1, 'S': 0, 'I': 0, 'U': 0}).fillna(0).astype(int)\n",
    "        df.drop(columns=['occpy_sts'], inplace=True)\n",
    "        print(f\"'occpy_sts' -> 'if_prim_res': {df['if_prim_res'].value_counts().to_dict()}\")\n",
    "\n",
    "    if 'channel' in df.columns:\n",
    "        df['if_corr'] = df['channel'].replace({'C': 1, 'R': 0, 'B': 0, 'T': 0}).fillna(0).astype(int)\n",
    "        df.drop(columns=['channel'], inplace=True)\n",
    "        print(f\"'channel' -> 'if_corr': {df['if_corr'].value_counts().to_dict()}\")\n",
    "\n",
    "    if 'prop_type' in df.columns:\n",
    "        df['if_sf'] = df['prop_type'].replace({'SF': 1, 'CO': 0, 'PU': 0, 'MH': 0, 'CP': 0}).fillna(0).astype(int)\n",
    "        df.drop(columns=['prop_type'], inplace=True)\n",
    "        print(f\"'prop_type' -> 'if_sf': {df['if_sf'].value_counts().to_dict()}\")\n",
    "\n",
    "    if 'loan_purpose' in df.columns:\n",
    "        df['if_purc'] = df['loan_purpose'].replace({'P': 1, 'N': 0, 'C': 0}).fillna(0).astype(int)\n",
    "        df.drop(columns=['loan_purpose'], inplace=True)\n",
    "        print(f\"'loan_purpose' -> 'if_purc': {df['if_purc'].value_counts().to_dict()}\")\n",
    "\n",
    "    if 'flag_sc' in df.columns:\n",
    "        df['if_sc'] = df['flag_sc'].replace({'Y': 1, 'N': 0}).fillna(0).astype(int)\n",
    "        df.drop(columns=['flag_sc'], inplace=True)\n",
    "        print(f\"'flag_sc' -> 'if_sc': {df['if_sc'].value_counts().to_dict()}\")\n",
    "\n",
    "    if 'loan_defaulted' in df.columns:\n",
    "        df['default'] = df['loan_defaulted'].fillna(0).astype(int)\n",
    "        df.drop(columns=['loan_defaulted'], inplace=True)\n",
    "        print(f\"'loan_defaulted' -> 'default': {df['default'].value_counts().to_dict()}\")\n",
    "\n",
    "    print(\"  - Processing numerical features:\")\n",
    "    numerical_features = [\n",
    "        'fico', 'mi_pct', 'cnt_units', 'dti', 'ltv',\n",
    "        'cnt_borr', 'orig_upb', 'current_upb'\n",
    "    ]\n",
    "\n",
    "    for col in [f for f in numerical_features if f in df.columns]:\n",
    "        df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "        if col == 'orig_upb' and (df[col] < 0).any():\n",
    "            neg_count = (df[col] < 0).sum()\n",
    "            median_val = df.loc[df[col] >= 0, col].median()\n",
    "            df.loc[df[col] < 0, col] = median_val if not pd.isna(median_val) else 0\n",
    "            print(f\"Corrected {neg_count} negative values in '{col}'\")\n",
    "        fill_val = df[col].median() if col not in ['fico', 'cnt_units', 'cnt_borr'] else df[col].mode()[0]\n",
    "        df[col] = df[col].fillna(fill_val)\n",
    "        if col in ['fico', 'cnt_units', 'cnt_borr']:\n",
    "            df[col] = df[col].round().astype(int)\n",
    "        print(f\"Processed '{col}': min={df[col].min():.2f}, max={df[col].max():.2f}\")\n",
    "\n",
    "    if 'zipcode' in df.columns:\n",
    "        df['zipcode'] = (\n",
    "            df['zipcode']\n",
    "            .astype(str)\n",
    "            .str.extract(r'(\\d{5})')[0]\n",
    "            .fillna('00000')\n",
    "        )\n",
    "        df['area'] = df['zipcode'].str[:2].replace('', '00').fillna('00')\n",
    "        df.drop('zipcode', axis=1, inplace=True)\n",
    "        print(f\"Created 'area' with values: {sorted(df['area'].unique())}\")\n",
    "\n",
    "    rename_map = {\n",
    "        'seller_name': 'provider',\n",
    "        'orig_loan_term': 'loan_term'\n",
    "    }\n",
    "    df.rename(columns={k: v for k, v in rename_map.items() if k in df.columns}, inplace=True)\n",
    "\n",
    "    if 'id' not in df.columns:\n",
    "        df['id'] = np.arange(1, len(df)+1)\n",
    "\n",
    "    if 'id_loan' in df.columns:\n",
    "        df['id_loan'] = df['id_loan'].astype(str)\n",
    "\n",
    "    df = df.loc[:, ~df.columns.duplicated()]\n",
    "\n",
    "    print(f\"Final DataFrame shape after cleaning: {df.shape}\")\n",
    "    print(\"### Finished cleaning and feature engineering ###\")\n",
    "    return df\n",
    "\n",
    "def apply_minmax_scaling(df_train: pd.DataFrame, df_test: pd.DataFrame) -> (pd.DataFrame, pd.DataFrame):\n",
    "\n",
    "    print(\"\\nStarting MinMax Scaling for Main Data\")\n",
    "\n",
    "    scaler = MinMaxScaler()\n",
    "\n",
    "    unscaled_cols = [\n",
    "        'id', 'id_loan', 'year', 'month', 'provider',\n",
    "        'area', 'default'\n",
    "    ]\n",
    "    if 'd_timer' in df_train.columns:\n",
    "        unscaled_cols.append('d_timer')\n",
    "\n",
    "    print(\"Processing training data:\")\n",
    "    df_train_processed = df_train.copy()\n",
    "\n",
    "    numeric_cols = [\n",
    "        col for col in df_train_processed.columns\n",
    "        if col not in unscaled_cols\n",
    "        and pd.api.types.is_numeric_dtype(df_train_processed[col])\n",
    "    ]\n",
    "    print(f\"Numeric columns identified for scaling (Training): {numeric_cols}\")\n",
    "\n",
    "    features_to_scale_train = df_train_processed[numeric_cols].copy()\n",
    "    for col in numeric_cols:\n",
    "        features_to_scale_train[col] = pd.to_numeric(\n",
    "            features_to_scale_train[col],\n",
    "            errors='coerce'\n",
    "        ).fillna(0)\n",
    "\n",
    "    non_numeric_check = features_to_scale_train.applymap(lambda x: isinstance(x, str)).any().any()\n",
    "    if non_numeric_check:\n",
    "        raise ValueError(\"Non-numeric values (strings) still present after conversion in features_to_scale_train. Check `fillna` logic.\")\n",
    "\n",
    "    try:\n",
    "        scaled_features_train = pd.DataFrame(\n",
    "            scaler.fit_transform(features_to_scale_train),\n",
    "            columns=features_to_scale_train.columns,\n",
    "            index=df_train_processed.index\n",
    "        )\n",
    "    except ValueError as e:\n",
    "        print(\"Error during scaling. Problematic columns in training data:\")\n",
    "        for col in numeric_cols:\n",
    "            unique_vals = df_train_processed[col].dropna().unique()\n",
    "            if any(isinstance(x, str) for x in unique_vals):\n",
    "                print(f\"- {col}: Contains string values - {unique_vals}\")\n",
    "        raise e\n",
    "\n",
    "    print(\"Processing testing data:\")\n",
    "    df_test_processed = df_test.copy()\n",
    "    scaled_features_test = pd.DataFrame()\n",
    "\n",
    "    if not df_test_processed.empty:\n",
    "        numeric_cols_test = [col for col in numeric_cols if col in df_test_processed.columns]\n",
    "        features_to_scale_test = df_test_processed[numeric_cols_test].copy()\n",
    "        for col in numeric_cols_test:\n",
    "            features_to_scale_test[col] = pd.to_numeric(\n",
    "                features_to_scale_test[col],\n",
    "                errors='coerce'\n",
    "            ).fillna(0)\n",
    "\n",
    "        scaled_features_test = pd.DataFrame(\n",
    "            scaler.transform(features_to_scale_test),\n",
    "            columns=features_to_scale_test.columns,\n",
    "            index=df_test_processed.index\n",
    "        )\n",
    "        scaled_features_test = np.clip(scaled_features_test, 0, 1)\n",
    "\n",
    "    unscaled_metadata_train = df_train_processed[[col for col in unscaled_cols if col in df_train_processed.columns]]\n",
    "    unscaled_metadata_test = df_test_processed[[col for col in unscaled_cols if col in df_test_processed.columns]]\n",
    "\n",
    "    df_train_scaled = pd.concat([unscaled_metadata_train, scaled_features_train], axis=1)\n",
    "    df_test_scaled = pd.concat([unscaled_metadata_test, scaled_features_test], axis=1)\n",
    "\n",
    "    print(\"### Finished MinMax Scaling for Main Data ###\")\n",
    "    return df_train_scaled, df_test_scaled\n",
    "\n",
    "def handle_missing_values(df, missing_threshold=0.8):\n",
    "\n",
    "    print(\"\\nHandling missing values\")\n",
    "    n = len(df.index)\n",
    "    high_missing_cols = []\n",
    "    for column in df.columns:\n",
    "        null_count = df[column].isnull().sum()\n",
    "        null_percentage = null_count / n\n",
    "        if null_percentage > missing_threshold:\n",
    "            high_missing_cols.append(column)\n",
    "            \n",
    "    if high_missing_cols:\n",
    "        df = df.drop(columns=high_missing_cols)\n",
    "        print(f\"Dropped {len(high_missing_cols)} columns with >{missing_threshold*100}% missing values: {high_missing_cols}\")\n",
    "        \n",
    "    for column in df.columns:\n",
    "        null_count = df[column].isnull().sum()\n",
    "        \n",
    "        if null_count > 0:\n",
    "            \n",
    "            if pd.api.types.is_numeric_dtype(df[column]):\n",
    "                df[column] = df[column].fillna(df[column].median())\n",
    "                print(f\"Filled {null_count} missing values in '{column}' with median\")\n",
    "            else:\n",
    "                mode_val = df[column].mode()\n",
    "                \n",
    "                if not mode_val.empty:\n",
    "                    df[column] = df[column].fillna(mode_val[0])\n",
    "                    print(f\"Filled {null_count} missing values in '{column}' with mode\")\n",
    "                else:\n",
    "                    df[column] = df[column].fillna('missing_category')\n",
    "                    print(f\"Filled {null_count} missing values in '{column}' with 'missing_category'\")\n",
    "                    \n",
    "    print(\"### Finished handling missing values ###\")\n",
    "    return df\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    pd.set_option('future.no_silent_downcasting', True)\n",
    "    PREPARED_DATA_DIR = '../data/prepared_data'\n",
    "    OUTPUT_CLEANED_DATA_DIR = '../data/cleaned_data'\n",
    "\n",
    "    if not os.path.exists(OUTPUT_CLEANED_DATA_DIR):\n",
    "        os.makedirs(OUTPUT_CLEANED_DATA_DIR)\n",
    "        print(f\"Created output directory for cleaned data: {OUTPUT_CLEANED_DATA_DIR}\")\n",
    "\n",
    "    train_main_file_path = os.path.join(PREPARED_DATA_DIR, 'prepared_2015_full_year_data.csv')\n",
    "    test_main_file_path = os.path.join(PREPARED_DATA_DIR, 'prepared_2016_full_year_data.csv')\n",
    "\n",
    "    df_train_main_raw = pd.DataFrame()\n",
    "    df_test_main_raw = pd.DataFrame()\n",
    "\n",
    "    print(\"\\n### Loading raw Main Data (Origination + Latest Performance) ###\")\n",
    "    if os.path.exists(train_main_file_path):\n",
    "        df_train_main_raw = pd.read_csv(train_main_file_path, low_memory=False)\n",
    "        df_train_main_raw = df_train_main_raw.reset_index(drop=True)\n",
    "        print(f\"Loaded training main data (2015) from {train_main_file_path}. Shape: {df_train_main_raw.shape}\")\n",
    "        if 'svcg_cycle' not in df_train_main_raw.columns:\n",
    "            print(f\"'svcg_cycle' column not found in training main data: {train_main_file_path} ***\")\n",
    "    else:\n",
    "        print(f\"Error: Training main data file (2015) not found at {train_main_file_path}.\")\n",
    "\n",
    "    if os.path.exists(test_main_file_path):\n",
    "        \n",
    "        df_test_main_raw = pd.read_csv(test_main_file_path, low_memory=False)\n",
    "        df_test_main_raw = df_test_main_raw.reset_index(drop=True)\n",
    "        print(f\"Loaded testing main data (2016) from {test_main_file_path}. Shape: {df_test_main_raw.shape}\")\n",
    "        \n",
    "        if 'svcg_cycle' not in df_test_main_raw.columns:\n",
    "            print(f\"Warning: 'svcg_cycle' column not found in testing main data: {test_main_file_path}\")\n",
    "    else:\n",
    "        print(f\"Error: Testing main data file (2016) not found at {test_main_file_path}.\")\n",
    "\n",
    "    if not df_train_main_raw.empty and not df_test_main_raw.empty:\n",
    "        \n",
    "        df_train_main_raw = handle_missing_values(df_train_main_raw)\n",
    "        df_test_main_raw = handle_missing_values(df_test_main_raw)\n",
    "        df_train_cleaned_full = clean_and_engineer_features(df_train_main_raw)\n",
    "        df_test_cleaned_full = clean_and_engineer_features(df_test_main_raw)\n",
    "\n",
    "        print(\"\\n### Applying temporal train/test/graph/explanation splits for Main Data ###\")\n",
    "\n",
    "        df_train_cleaned_full['dt_orig_dt'] = pd.to_datetime(df_train_cleaned_full['dt_orig'], format='%Y%m', errors='coerce').dt.to_period('M').dt.to_timestamp()\n",
    "        df_test_cleaned_full['dt_orig_dt'] = pd.to_datetime(df_test_cleaned_full['dt_orig'], format='%Y%m', errors='coerce').dt.to_period('M').dt.to_timestamp()\n",
    "        df_train_cleaned_full = df_train_cleaned_full.dropna(subset=['dt_orig_dt']).copy()\n",
    "        df_test_cleaned_full = df_test_cleaned_full.dropna(subset=['dt_orig_dt']).copy()\n",
    "        train_graph_start_date = datetime(2015, 1, 1)\n",
    "        train_graph_end_date = datetime(2015, 6, 30)\n",
    "        train_explanation_start_date = datetime(2015, 7, 1)\n",
    "        train_explanation_end_date = datetime(2015, 7, 31)\n",
    "        test_graph_start_date = datetime(2016, 1, 1)\n",
    "        test_graph_end_date = datetime(2016, 6, 30)\n",
    "        test_explanation_start_date = datetime(2016, 7, 1)\n",
    "        test_explanation_end_date = datetime(2016, 7, 31)\n",
    "\n",
    "        df_train_graph_cleaned = df_train_cleaned_full[\n",
    "            (df_train_cleaned_full['dt_orig_dt'] >= train_graph_start_date) &\n",
    "            (df_train_cleaned_full['dt_orig_dt'] <= train_graph_end_date)\n",
    "        ].copy()\n",
    "        df_train_graph_cleaned = df_train_graph_cleaned.reset_index(drop=True)\n",
    "        print(f\"Train Graph Data (2015 Jan-Jun) raw shape: {df_train_graph_cleaned.shape}\")\n",
    "\n",
    "        df_test_graph_cleaned = df_test_cleaned_full[\n",
    "            (df_test_cleaned_full['dt_orig_dt'] >= test_graph_start_date) &\n",
    "            (df_test_cleaned_full['dt_orig_dt'] <= test_graph_end_date)\n",
    "        ].copy()\n",
    "        df_test_graph_cleaned = df_test_graph_cleaned.reset_index(drop=True)\n",
    "        print(f\"Test Graph Data (2016 Jan-Jun) raw shape: {df_test_graph_cleaned.shape}\")\n",
    "\n",
    "        target_test_graph_nodes = 5000\n",
    "        if len(df_test_graph_cleaned) > target_test_graph_nodes:\n",
    "            df_test_graph_cleaned = df_test_graph_cleaned.sample(n=target_test_graph_nodes, random_state=42).copy()\n",
    "            df_test_graph_cleaned = df_test_graph_cleaned.reset_index(drop=True)\n",
    "            print(f\"Downsampled Test Graph Data (2016 Jan-Jun) to {target_test_graph_nodes} nodes.\")\n",
    "        elif len(df_test_graph_cleaned) < target_test_graph_nodes:\n",
    "            print(f\"Test Graph Data (2016 Jan-Jun) has {len(df_test_graph_cleaned)} nodes, which is less than target {target_test_graph_nodes}. Using all available.\")\n",
    "\n",
    "        print(f\"### Train Graph Data (2015 Jan-Jun) final shape: {df_train_graph_cleaned.shape} ###\")\n",
    "        print(f\"### Test Graph Data (2016 Jan-Jun) final shape: {df_test_graph_cleaned.shape} ###\")\n",
    "\n",
    "        df_train_explanation_cleaned = df_train_cleaned_full[\n",
    "            (df_train_cleaned_full['dt_orig_dt'] >= train_explanation_start_date) &\n",
    "            (df_train_cleaned_full['dt_orig_dt'] <= train_explanation_end_date)\n",
    "        ].copy()\n",
    "        df_train_explanation_cleaned = df_train_explanation_cleaned.reset_index(drop=True)\n",
    "        print(f\"Train Explanation Data (2015 July) shape: {df_train_explanation_cleaned.shape}\")\n",
    "\n",
    "        df_test_explanation_cleaned = df_test_cleaned_full[\n",
    "            (df_test_cleaned_full['dt_orig_dt'] >= test_explanation_start_date) &\n",
    "            (df_test_cleaned_full['dt_orig_dt'] <= test_explanation_end_date)\n",
    "        ].copy()\n",
    "        df_test_explanation_cleaned = df_test_explanation_cleaned.reset_index(drop=True)\n",
    "        print(f\"Test Explanation Data (2016 July) shape: {df_test_explanation_cleaned.shape}\")\n",
    "\n",
    "        df_train_graph_cleaned.drop(columns=['dt_orig_dt'], inplace=True)\n",
    "        df_test_graph_cleaned.drop(columns=['dt_orig_dt'], inplace=True)\n",
    "        df_train_explanation_cleaned.drop(columns=['dt_orig_dt'], inplace=True)\n",
    "        df_test_explanation_cleaned.drop(columns=['dt_orig_dt'], inplace=True)\n",
    "\n",
    "        print(\"\\n### Applying MinMax Scaling to prepared data ###\")\n",
    "\n",
    "        scaler = MinMaxScaler()\n",
    "\n",
    "        unscaled_cols = [\n",
    "            'id', 'id_loan', 'year', 'month', 'provider',\n",
    "            'area', 'default', 'd_timer'\n",
    "        ]\n",
    "\n",
    "        numeric_cols_for_scaling = [\n",
    "            col for col in df_train_graph_cleaned.columns\n",
    "            if col not in unscaled_cols\n",
    "            and pd.api.types.is_numeric_dtype(df_train_graph_cleaned[col])\n",
    "        ]\n",
    "        print(f\"Master numeric columns for scaling: {numeric_cols_for_scaling}\")\n",
    "\n",
    "        train_graph_features = df_train_graph_cleaned[numeric_cols_for_scaling].copy()\n",
    "        for col in numeric_cols_for_scaling:\n",
    "            train_graph_features[col] = pd.to_numeric(train_graph_features[col], errors='coerce').fillna(0)\n",
    "        scaler.fit(train_graph_features)\n",
    "        print(\"Scaler fitted on df_train_graph_cleaned (Jan-June 2015 data).\")\n",
    "\n",
    "        def transform_dataframe(df_to_transform, scaler, numeric_cols, unscaled_cols_list):\n",
    "            \n",
    "            df_transformed = df_to_transform.copy()\n",
    "            features_to_scale = df_transformed[numeric_cols].copy()\n",
    "            \n",
    "            for col in numeric_cols:\n",
    "                features_to_scale[col] = pd.to_numeric(features_to_scale[col], errors='coerce').fillna(0)\n",
    "\n",
    "            scaled_features = pd.DataFrame(\n",
    "                scaler.transform(features_to_scale),\n",
    "                columns=features_to_scale.columns,\n",
    "                index=df_transformed.index\n",
    "            )\n",
    "            \n",
    "            scaled_features = np.clip(scaled_features, 0, 1)\n",
    "            unscaled_metadata = df_transformed[[col for col in unscaled_cols_list if col in df_transformed.columns]]\n",
    "            \n",
    "            return pd.concat([unscaled_metadata, scaled_features], axis=1)\n",
    "\n",
    "        df_train_graph_scaled = transform_dataframe(df_train_graph_cleaned, scaler, numeric_cols_for_scaling, unscaled_cols)\n",
    "        df_test_graph_scaled = transform_dataframe(df_test_graph_cleaned, scaler, numeric_cols_for_scaling, unscaled_cols)\n",
    "        df_train_explanation_scaled = transform_dataframe(df_train_explanation_cleaned, scaler, numeric_cols_for_scaling, unscaled_cols)\n",
    "        df_test_explanation_scaled = transform_dataframe(df_test_explanation_cleaned, scaler, numeric_cols_for_scaling, unscaled_cols)\n",
    "\n",
    "        print(\"\\ndtypes from scaled data (before saving)\")\n",
    "        sample_cols_to_check = ['fico', 'mi_pct', 'cnt_units', 'cnt_borr', 'orig_upb', 'current_upb', 'd_timer']\n",
    "        \n",
    "        for df_name, df_data in [\n",
    "            (\"df_train_graph_scaled\", df_train_graph_scaled),\n",
    "            (\"df_test_graph_scaled\", df_test_graph_scaled),\n",
    "            (\"df_train_explanation_scaled\", df_train_explanation_scaled),\n",
    "            (\"df_test_explanation_scaled\", df_test_explanation_scaled)\n",
    "        ]:\n",
    "            present_cols = [col for col in sample_cols_to_check if col in df_data.columns]\n",
    "            if present_cols:\n",
    "                print(f\"{df_name} dtypes (selected):\")\n",
    "                print(df_data[present_cols].dtypes)\n",
    "            else:\n",
    "                print(f\"No selected columns found in {df_name} for dtype check.\")\n",
    "\n",
    "        df_train_graph_scaled.to_csv(os.path.join(OUTPUT_CLEANED_DATA_DIR, 'df_origination_train_graph_scaled.csv'), index=False)\n",
    "        print(f\"\\nSaved scaled training graph data (Jan-Jun 2015) to {os.path.join(OUTPUT_CLEANED_DATA_DIR, 'df_origination_train_graph_scaled.csv')}\")\n",
    "\n",
    "        df_test_graph_scaled.to_csv(os.path.join(OUTPUT_CLEANED_DATA_DIR, 'df_origination_test_graph_scaled.csv'), index=False)\n",
    "        print(f\"Saved scaled testing graph data (Jan-Jun 2016) to {os.path.join(OUTPUT_CLEANED_DATA_DIR, 'df_origination_test_graph_scaled.csv')}\")\n",
    "\n",
    "        df_train_explanation_scaled.to_csv(os.path.join(OUTPUT_CLEANED_DATA_DIR, 'df_origination_train_explanation_scaled.csv'), index=False)\n",
    "        print(f\"Saved scaled training explanation data (July 2015) to {os.path.join(OUTPUT_CLEANED_DATA_DIR, 'df_origination_train_explanation_scaled.csv')}\")\n",
    "\n",
    "        df_test_explanation_scaled.to_csv(os.path.join(OUTPUT_CLEANED_DATA_DIR, 'df_origination_test_explanation_scaled.csv'), index=False)\n",
    "        print(f\"Saved scaled testing explanation data (July 2016) to {os.path.join(OUTPUT_CLEANED_DATA_DIR, 'df_origination_test_explanation_scaled.csv')}\")\n",
    "\n",
    "    else:\n",
    "        print(\"Error: Cannot proceed with cleaning and scaling due to missing input data.\")\n",
    "\n",
    "    print(\"\\n### Data Cleaning and Temporal Splitting complete! ###\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (llmvenv)",
   "language": "python",
   "name": "llmvenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
