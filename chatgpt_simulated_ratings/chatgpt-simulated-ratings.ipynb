{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "69b6388c-f7dc-4d45-ac45-97bc27d90e0e",
   "metadata": {},
   "source": [
    "## Automated Evaluation of AI-Generated Loan Decision Explanations Using Simulated Expert Assessment\n",
    "\n",
    "This notebook implements an automated evaluation framework that uses GPT-4o to simulate both Credit Risk Professional (CRP) and Non-Credit Risk Professional (NCRP) personas for rating AI-generated loan decision explanations across 8 key metrics: Understandability, Trustworthiness, Insightfulness, Satisfaction, Confidence, Convincingness, Communicability, and Usability.\n",
    "\n",
    "Note: This script is intended for academic reference only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "117eaa68-0f31-483d-9c83-dad1c881d2dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import scipy.stats as stats\n",
    "\n",
    "import openai\n",
    "from tenacity import retry, stop_after_attempt, wait_exponential\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "import os\n",
    "import json\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "client = openai.OpenAI(api_key=api_key)\n",
    "\n",
    "METRICS = {\n",
    "    \"Understandability\": {\n",
    "        \"crp\": \"The explanation is easy to understand.\",\n",
    "        \"ncrp\": \"The explanation is easy to understand.\"\n",
    "    },\n",
    "    \"Trustworthiness\": {\n",
    "        \"crp\": \"The explanation can be trusted because it presents sufficient and reliable evidence to support the loan decision.\",\n",
    "        \"ncrp\": \"I trust this explanation given by the model.\"\n",
    "    },\n",
    "    \"Insightfulness\": {\n",
    "        \"crp\": \"This explanation reveals insightful risk factors that influence loan decisions.\",\n",
    "        \"ncrp\": \"This explanation provides useful insight into why the decision was made.\"\n",
    "    },\n",
    "    \"Satisfaction\": {\n",
    "        \"crp\": \"The level of details satisfies my expectations for a risk assessment.\",\n",
    "        \"ncrp\": \"I am satisfied with how the explanation addressed my concerns about the decision.\"\n",
    "    },\n",
    "    \"Confidence\": {\n",
    "        \"crp\": \"This explanation increases my confidence for loan approval/denial.\",\n",
    "        \"ncrp\": \"I am confident in the explanation provided for the model's decision.\"\n",
    "    },\n",
    "    \"Convincingness\": {\n",
    "        \"crp\": \"The justification is convincing and argues for the decision using well-weighted risk evidence.\",\n",
    "        \"ncrp\": \"The explanation is convincing and makes the decision seem reasonable.\"\n",
    "    },\n",
    "    \"Communicability\": {\n",
    "        \"crp\": \"I could use this explanation to communicate with customers and other teams in the company.\",\n",
    "        \"ncrp\": \"The explanation is clear enough for me to communicate with others.\"\n",
    "    },\n",
    "    \"Usability\": {\n",
    "        \"crp\": \"This explanation could be directly used in loan approval/denial.\",\n",
    "        \"ncrp\": \"I am likely to use this explanation in the future because it supports my decision-making.\"\n",
    "    }\n",
    "}\n",
    "\n",
    "ACRONYMS = {\n",
    "    \"Understandability\": \"UND\",\n",
    "    \"Trustworthiness\": \"TRU\", \n",
    "    \"Insightfulness\": \"INS\",\n",
    "    \"Satisfaction\": \"SAT\",\n",
    "    \"Confidence\": \"CON\",\n",
    "    \"Convincingness\": \"CVN\",\n",
    "    \"Communicability\": \"COM\",\n",
    "    \"Usability\": \"USB\"\n",
    "}\n",
    "\n",
    "@retry(stop=stop_after_attempt(5), wait=wait_exponential(min=3, max=30))\n",
    "def get_likert_scores(explanation, persona=\"crp\"):\n",
    "    \n",
    "    criteria = \"\\n\".join([f\"{i+1}. {METRICS[m][persona]}\" for i, m in enumerate(METRICS)])\n",
    "    scale_definition = (\n",
    "        \"**Use the following scale for all ratings:**\\n\"\n",
    "        \"- 1 = Strongly Disagree\\n\"\n",
    "        \"- 2 = Disagree\\n\"\n",
    "        \"- 3 = Neutral\\n\"\n",
    "        \"- 4 = Agree\\n\"\n",
    "        \"- 5 = Strongly Agree\"\n",
    "    )\n",
    "    \n",
    "    if persona == \"crp\":\n",
    "        role_description = (\n",
    "            \"You are a Credit Risk Professional evaluating AI-generated explanations. \"\n",
    "            \"Assess each explanation as if you were reviewing it in a formal credit-risk setting. \"\n",
    "            \"Consider evidential support, decision justification, and operational relevance.\"\n",
    "        )\n",
    "    else:\n",
    "        role_description = (\n",
    "            \"You are a general Non-Credit Risk Professional evaluating AI-generated explanations. \"\n",
    "            \"Assess the explanation from a general reader's perspective, focusing on clarity, \"\n",
    "            \"perceived insight, and general usefulness without requiring credit-specific expertise.\"\n",
    "        )\n",
    "    \n",
    "    input_text = f\"\"\"{role_description}\n",
    "\n",
    "Rate the following explanation based on these 8 criteria:\n",
    "{criteria}\n",
    "{scale_definition}\n",
    "\n",
    "Return a JSON dictionary with numeric keys (\"1\", \"2\", \"3\", etc.) mapping to scores from 1 to 5 (integers only). \n",
    "Do not include commentary, explanation, or extra formatting—return only the JSON object.\n",
    "\n",
    "Explanation:\n",
    "\\\"\\\"\\\"{explanation}\\\"\\\"\\\"\"\"\"\n",
    "    \n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-4o\",\n",
    "            messages=[system_msg, user_msg],\n",
    "            temperature=0.2,\n",
    "            max_tokens=1000,\n",
    "            response_format={\"type\": \"json_object\"}\n",
    "        )\n",
    "\n",
    "        content = response.choices[0].message.content\n",
    "        \n",
    "        raw_scores = json.loads(content)\n",
    "        \n",
    "        metric_names = list(METRICS.keys())\n",
    "        converted_scores = {}\n",
    "        \n",
    "        for i, metric_name in enumerate(metric_names):\n",
    "            key = str(i + 1) \n",
    "            if key in raw_scores:\n",
    "                converted_scores[metric_name] = raw_scores[key]\n",
    "        \n",
    "        return converted_scores\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error in API call: {e}\")\n",
    "        return {} \n",
    "\n",
    "def compute_ci(scores):\n",
    "\n",
    "    mean = np.mean(scores)\n",
    "    stderr = stats.sem(scores)\n",
    "    margin = stderr * stats.t.ppf((1 + 0.95) / 2, len(scores) - 1)\n",
    "    ci_low = max(1.0, mean - margin)\n",
    "    ci_high = min(5.0, mean + margin)\n",
    "    return mean, ci_low, ci_high\n",
    "\n",
    "def print_round_scores(round_num, crp_scores, ncrp_scores):\n",
    "    \n",
    "    print(f\"\\n### ROUND {round_num} ###\\n\")\n",
    "    \n",
    "    print(\"CRP:\", end=\" \")\n",
    "    crp_values = []\n",
    "    for metric in METRICS:\n",
    "        acronym = ACRONYMS[metric]\n",
    "        score = crp_scores.get(metric, 0)\n",
    "        crp_values.append(f\"{acronym}:{score}\")\n",
    "    print(\" \".join(crp_values))\n",
    "    \n",
    "    print(\"NCRP:\", end=\" \")\n",
    "    ncrp_values = []\n",
    "    for metric in METRICS:\n",
    "        acronym = ACRONYMS[metric]\n",
    "        score = ncrp_scores.get(metric, 0)\n",
    "        ncrp_values.append(f\"{acronym}:{score}\")\n",
    "    print(\" \".join(ncrp_values))\n",
    "\n",
    "def print_final_summary(all_crp_scores, all_ncrp_scores):\n",
    "\n",
    "    print(\"### Final Summary with 95% Confidence Intervals ###\")\n",
    "    \n",
    "    print(\"\\nCRP Summary:\")\n",
    "    for metric in METRICS:\n",
    "        acronym = ACRONYMS[metric]\n",
    "        scores = all_crp_scores[metric]\n",
    "        if len(scores) >= 2:\n",
    "            mean, ci_low, ci_high = compute_ci(scores)\n",
    "            margin = ci_high - mean\n",
    "            print(f\"{acronym}: {mean:.2f} ± {margin:.2f}\")\n",
    "        else:\n",
    "            print(f\"{acronym}: N/A\")\n",
    "    \n",
    "    print(\"\\nNCRP Summary:\")\n",
    "    for metric in METRICS:\n",
    "        acronym = ACRONYMS[metric]\n",
    "        scores = all_ncrp_scores[metric]\n",
    "        if len(scores) >= 2:\n",
    "            mean, ci_low, ci_high = compute_ci(scores)\n",
    "            margin = ci_high - mean\n",
    "            print(f\"{acronym}: {mean:.2f} ± {margin:.2f}\")\n",
    "        else:\n",
    "            print(f\"{acronym}: N/A\")\n",
    "\n",
    "def run_eval(path, max_samples=50):\n",
    "\n",
    "    with open(path, \"r\") as f:\n",
    "        lines = [json.loads(line) for line in f][:max_samples]\n",
    "    \n",
    "    print(f\"Running ChatGPT Simulated Ratings on {len(lines)} samples\\n\")\n",
    "    \n",
    "    all_crp_scores = {metric: [] for metric in METRICS}\n",
    "    all_ncrp_scores = {metric: [] for metric in METRICS}\n",
    "    \n",
    "    for i, example in enumerate(lines, 1):\n",
    "        print(f\"\\nProcessing explanation {i}/{len(lines)}...\")\n",
    "        \n",
    "        crp_scores = get_likert_scores(example[\"explanation\"], \"crp\")\n",
    "        print(\"CRP scores retrieved successfully\")\n",
    "            \n",
    "        ncrp_scores = get_likert_scores(example[\"explanation\"], \"ncrp\")\n",
    "        print(\"NCRP scores retrieved successfully\")\n",
    "        \n",
    "        for metric in METRICS:\n",
    "            if metric in crp_scores:\n",
    "                all_crp_scores[metric].append(crp_scores[metric])\n",
    "            if metric in ncrp_scores:\n",
    "                all_ncrp_scores[metric].append(ncrp_scores[metric])\n",
    "        \n",
    "        print_round_scores(i, crp_scores, ncrp_scores)\n",
    "    \n",
    "    print_final_summary(all_crp_scores, all_ncrp_scores)\n",
    "    \n",
    "    return all_crp_scores, all_ncrp_scores\n",
    "\n",
    "\n",
    "# print(\"Evaluating Gemini-XGB\")\n",
    "# print(\"Evaluating Gemini-GAT\")\n",
    "# print(\"Evaluating Gemini-Hybrid\")\n",
    "# print(\"Evaluating Gemma3-XGB\")\n",
    "# print(\"Evaluating Gemma3-GAT\")\n",
    "# print(\"Evaluating Gemma3-Hybrid\")\n",
    "# print(\"Evaluating DeepSeek-R1-XGB\")\n",
    "# print(\"Evaluating DeepSeek-R1-GAT\")\n",
    "print(\"Evaluating DeepSeek-R1-Hybrid\")\n",
    "\n",
    "# EXPLANATION_PATH = \"../models/explanations/gemini_xgb_explanations.jsonl\"\n",
    "# EXPLANATION_PATH = \"../models/explanations/gemini_gnn_explanations.jsonl\"\n",
    "# EXPLANATION_PATH = \"../models/explanations/gemini_hybrid_explanations.jsonl\"\n",
    "# EXPLANATION_PATH = \"../models/explanations/gemma_xgb_explanations.jsonl\"\n",
    "# EXPLANATION_PATH = \"../models/explanations/gemma_gnn_explanations.jsonl\"\n",
    "# EXPLANATION_PATH = \"../models/explanations/gemma_hybrid_explanations.jsonl\"\n",
    "# EXPLANATION_PATH = \"../models/explanations/deepseek_xgb_explanations.jsonl\"\n",
    "# EXPLANATION_PATH = \"../models/explanations/deepseek_gnn_explanations.jsonl\"\n",
    "EXPLANATION_PATH = \"../models/explanations/deepseek_hybrid_explanations.jsonl\"\n",
    "\n",
    "crp_scores, ncrp_scores = run_eval(EXPLANATION_PATH, max_samples=100) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (shap-env)",
   "language": "python",
   "name": "shap-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
