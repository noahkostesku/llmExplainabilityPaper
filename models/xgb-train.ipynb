{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "65d7fadf-c277-48f2-ad65-26c15ad4df74",
   "metadata": {},
   "source": [
    "## XGBoost Model Training and SHAP-Based Evaluation for Loan Default Prediction\n",
    "\n",
    "This notebook trains an XGBoost model to predict loan defaults using tabular data. It includes hyperparameter tuning, performance evaluation (AUC, F1, precision, recall), and SHAP-based model interpretability. Key outputs like plots, metrics, and explanations are saved for analysis.\n",
    "\n",
    "Note: This script is intended for academic reference only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0645feea-7db1-4a59-8868-6969ea902130",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "\n",
    "import xgboost as xgb\n",
    "\n",
    "import os\n",
    "import random\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import (\n",
    "    roc_auc_score,\n",
    "    accuracy_score,\n",
    "    precision_score,\n",
    "    average_precision_score,\n",
    "    recall_score,\n",
    "    confusion_matrix,\n",
    "    ConfusionMatrixDisplay,\n",
    "    precision_recall_curve,\n",
    "    roc_curve,\n",
    "    classification_report\n",
    ")\n",
    "from sklearn.utils import resample\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "MODEL_FILE_NAME = 'trained_xgb_model.pkl'\n",
    "MODEL_SAVE_DIR = \"./\"\n",
    "MODEL_PATH = os.path.join(MODEL_SAVE_DIR, MODEL_FILE_NAME)\n",
    "EVAL_SAVE_DIR = '../evaluations/xgb_base_model_performance/'\n",
    "\n",
    "os.makedirs(EVAL_SAVE_DIR, exist_ok=True)\n",
    "\n",
    "def load_data():\n",
    "\n",
    "    train = pd.read_csv('../data/cleaned_data/df_origination_train_scaled.csv')\n",
    "    test = pd.read_csv('../data/cleaned_data/df_origination_test_scaled.csv')\n",
    "    exclude_cols = ['id', 'id_loan', 'year', 'month', 'provider', 'area', 'svcg_cycle']\n",
    "    target_col = 'default'\n",
    "    \n",
    "    if target_col not in train.columns and 'loan_defaulted' in train.columns:\n",
    "        target_col = 'loan_defaulted'\n",
    "    \n",
    "    if 'd_timer' in train.columns:\n",
    "        exclude_cols.append('d_timer')\n",
    "\n",
    "    X_train = train.drop(columns=[col for col in exclude_cols if col in train.columns] + [target_col])\n",
    "    y_train = train[target_col]\n",
    "    print(f\"Target variable distribution in training data:\\n{y_train.value_counts(normalize=True)}\")\n",
    "\n",
    "    X_test = test.drop(columns=[col for col in exclude_cols if col in test.columns] + [target_col])\n",
    "    y_test = test[target_col]\n",
    "\n",
    "    print(\"Data loaded successfully.\")\n",
    "    print(f\"Features: {X_train.shape[1]}\")\n",
    "    print(f\"Feature names: {', '.join(X_train.columns[:5])}... (and {X_train.shape[1]-5} more)\")\n",
    "    return X_train, y_train, X_test, y_test\n",
    "\n",
    "def train_model(X_train, y_train):\n",
    "\n",
    "    neg_count = (y_train == 0).sum()\n",
    "    pos_count = (y_train == 1).sum()\n",
    "    scale_pos_weight_value = neg_count / pos_count\n",
    "\n",
    "    print(f\"Calculated scale_pos_weight: {scale_pos_weight_value:.2f}\")\n",
    "\n",
    "    model = xgb.XGBClassifier(\n",
    "        objective='binary:logistic',\n",
    "        eval_metric='auc',\n",
    "        random_state=42,\n",
    "        gamma=0.001,\n",
    "        n_jobs=-1,\n",
    "    )\n",
    "\n",
    "    param_grid = {\n",
    "        'n_estimators': [50, 100, 150],\n",
    "        'max_depth': [2, 3, 4],\n",
    "        'learning_rate': [0.01, 0.1, 0.2],\n",
    "        'subsample': [0.8, 1.0],\n",
    "        'colsample_bytree': [0.8, 1.0],\n",
    "        'scale_pos_weight': [1, 2, 3, 5, 8, 10, scale_pos_weight_value]\n",
    "    }\n",
    "\n",
    "    grid_search = GridSearchCV(\n",
    "        estimator=model,\n",
    "        param_grid=param_grid,\n",
    "        scoring='roc_auc',\n",
    "        cv=5,\n",
    "        n_jobs=-1,\n",
    "        verbose=1,\n",
    "    )\n",
    "\n",
    "    print(\"Starting GridSearchCV for XGBoost model...\")\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    \n",
    "    print(f\"Best parameters found: {grid_search.best_params_}\")\n",
    "    print(f\"Best cross-validation AUC: {grid_search.best_score_:.4f}\")\n",
    "\n",
    "    return grid_search.best_estimator_\n",
    "\n",
    "def optimize_f1_threshold(model, X_test, y_test, eval_save_dir):\n",
    "\n",
    "    y_probs = model.predict_proba(X_test)[:, 1]\n",
    "    precision, recall, thresholds = precision_recall_curve(y_test, y_probs)\n",
    "    f1_scores = 2 * (precision * recall) / (precision + recall + 1e-8)\n",
    "    best_idx = np.argmax(f1_scores)\n",
    "    best_threshold = thresholds[best_idx]\n",
    "    best_f1 = f1_scores[best_idx]\n",
    "\n",
    "    print(f\"\\n### F1-score Optimization ###\")\n",
    "    print(f\"Optimized F1-score: {best_f1:.4f} at threshold: {best_threshold:.4f}\")\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.plot(thresholds, f1_scores[:-1], label=\"F1-score\")\n",
    "    plt.axvline(best_threshold, color='red', linestyle='--', label=f'Best threshold = {best_threshold:.4f}')\n",
    "    plt.xlabel(\"Threshold\")\n",
    "    plt.ylabel(\"F1-score\")\n",
    "    plt.title(\"F1-score vs. Classification Threshold\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.savefig(os.path.join(eval_save_dir, 'f1_threshold_plot.png'))\n",
    "    plt.close()\n",
    "    print(f\"F1-score vs. Threshold plot saved to {os.path.join(eval_save_dir, 'f1_threshold_plot.png')}\")\n",
    "\n",
    "    y_pred_optimized = (y_probs >= best_threshold).astype(int)\n",
    "    print(\"\\n### Classification Report at Optimized Threshold ###\")\n",
    "    print(classification_report(y_test, y_pred_optimized, target_names=['No Default', 'Default']))\n",
    "\n",
    "    return best_threshold, best_f1\n",
    "\n",
    "def evaluate_model_metrics_and_plots(model, X_test, y_test, threshold, eval_save_dir):\n",
    "\n",
    "    y_proba = model.predict_proba(X_test)[:, 1]\n",
    "    y_pred = (y_proba >= threshold).astype(int)\n",
    "    auc = roc_auc_score(y_test, y_proba)\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    prec = precision_score(y_test, y_pred, zero_division=0)\n",
    "    rec = recall_score(y_test, y_pred, zero_division=0)\n",
    "    f1 = 2 * (prec * rec) / (prec + rec + 1e-8)\n",
    "\n",
    "    print(f\"\\n### Test Performance (Threshold = {threshold:.2f}) ###\")\n",
    "    print(f\"AUC:         {auc:.4f}\")\n",
    "    print(f\"Accuracy:    {acc:.4f}\")\n",
    "    print(f\"Precision:   {prec:.4f}\")\n",
    "    print(f\"Recall:      {rec:.4f}\")\n",
    "    print(f\"F1-Score:    {f1:.4f}\")\n",
    "\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=['No Default', 'Default'])\n",
    "    fig, ax = plt.subplots(figsize=(6, 6))\n",
    "    disp.plot(ax=ax, cmap='Blues')\n",
    "    plt.title(f\"Confusion Matrix (Threshold = {threshold:.2f})\")\n",
    "    plt.savefig(os.path.join(eval_save_dir, 'confusion_matrix.png'))\n",
    "    plt.close()\n",
    "    print(f\"Confusion Matrix saved to {os.path.join(eval_save_dir, 'confusion_matrix.png')}\")\n",
    "\n",
    "    fpr, tpr, _ = roc_curve(y_test, y_proba)\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'AUC = {auc:.4f}')\n",
    "    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('ROC Curve')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.grid(True)\n",
    "    plt.savefig(os.path.join(eval_save_dir, 'roc_curve.png'))\n",
    "    plt.close()\n",
    "    print(f\"ROC curve saved to {os.path.join(eval_save_dir, 'roc_curve.png')}\")\n",
    "\n",
    "    precision, recall, _ = precision_recall_curve(y_test, y_proba)\n",
    "    pr_auc = average_precision_score(y_test, y_proba)\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.plot(recall, precision, color='purple', lw=2, label=f'PR AUC = {pr_auc:.4f}')\n",
    "    plt.xlabel('Recall')\n",
    "    plt.ylabel('Precision')\n",
    "    plt.title('Precision-Recall Curve')\n",
    "    plt.grid(True)\n",
    "    plt.legend()\n",
    "    plt.savefig(os.path.join(eval_save_dir, 'precision_recall_curve.png'))\n",
    "    plt.close()\n",
    "    print(f\"Precision-Recall curve saved to {os.path.join(eval_save_dir, 'precision_recall_curve.png')}\")\n",
    "\n",
    "def bootstrap_evaluation(model, X_test, y_test, n_iterations=1000):\n",
    "\n",
    "    auc_scores = []\n",
    "    print(f\"\\nStarting bootstrap evaluation ({n_iterations} iterations)...\")\n",
    "    for i in range(n_iterations):\n",
    "        X_resampled, y_resampled = resample(X_test, y_test, random_state=i)\n",
    "        y_proba = model.predict_proba(X_resampled)[:, 1]\n",
    "        auc_scores.append(roc_auc_score(y_resampled, y_proba))\n",
    "\n",
    "    mean_auc = np.mean(auc_scores)\n",
    "    ci_lower = np.percentile(auc_scores, 2.5)\n",
    "    ci_upper = np.percentile(auc_scores, 97.5)\n",
    "\n",
    "    print(f\"\\n### Bootstrap Results ({n_iterations} iterations) ###\")\n",
    "    print(f\"Mean AUC: {mean_auc:.4f}\")\n",
    "    print(f\"95% CI: [{ci_lower:.4f}, {ci_upper:.4f}]\")\n",
    "\n",
    "def get_top_shap_features(model, X_data, top_k=10):\n",
    "\n",
    "    explainer = shap.TreeExplainer(model)\n",
    "    shap_values = explainer.shap_values(X_data)\n",
    "    if isinstance(shap_values, list):\n",
    "        shap_values = shap_values[1]\n",
    "\n",
    "    mean_abs_shap = np.abs(shap_values).mean(axis=0)\n",
    "\n",
    "    feature_importance = pd.DataFrame({\n",
    "        'feature': X_data.columns,\n",
    "        'importance': mean_abs_shap\n",
    "    }).sort_values(by='importance', ascending=False)\n",
    "\n",
    "    return feature_importance.head(top_k).to_dict(orient='records')\n",
    "\n",
    "def shap_analysis_global(model, X_train, eval_save_dir):\n",
    "\n",
    "    explainer = shap.TreeExplainer(model)\n",
    "    shap_values_train = explainer.shap_values(X_train)\n",
    "\n",
    "    if isinstance(shap_values_train, list):\n",
    "        shap_values_train = shap_values_train[1]\n",
    "\n",
    "    print(\"\\nGenerating Global SHAP plots...\")\n",
    "\n",
    "    shap.summary_plot(shap_values_train, X_train, plot_type=\"bar\", show=False)\n",
    "    plt.title(\"Global Feature Importance (Mean Absolute SHAP)\")\n",
    "    plt.savefig(os.path.join(eval_save_dir, 'shap_global_bar.png'), bbox_inches='tight')\n",
    "    plt.close()\n",
    "    print(f\"SHAP global bar plot saved to {os.path.join(eval_save_dir, 'shap_global_bar.png')}\")\n",
    "\n",
    "    shap.summary_plot(shap_values_train, X_train, show=False)\n",
    "    plt.title(\"SHAP Feature Impact Summary\")\n",
    "    plt.savefig(os.path.join(eval_save_dir, 'shap_summary_beeswarm.png'), bbox_inches='tight')\n",
    "    plt.close()\n",
    "    print(f\"SHAP summary beeswarm plot saved to {os.path.join(eval_save_dir, 'shap_summary_beeswarm.png')}\")\n",
    "\n",
    "def export_shap_insights_for_single_borrower(model, X_test, y_test, optimized_threshold, num_top_features=5):\n",
    "\n",
    "    explainer = shap.TreeExplainer(model)\n",
    "    random_idx = random.choice(X_test.index.tolist())\n",
    "    single_instance = X_test.loc[[random_idx]]\n",
    "    single_prediction_shap_values = explainer.shap_values(single_instance)\n",
    "\n",
    "    if isinstance(single_prediction_shap_values, list):\n",
    "        single_prediction_shap_values = single_prediction_shap_values[1]\n",
    "\n",
    "    single_shap_dict = {\n",
    "        'index': int(random_idx),\n",
    "        'predicted_proba': float(model.predict_proba(single_instance)[:, 1][0]),\n",
    "        'actual_label': int(y_test.loc[random_idx]),\n",
    "        'top_features_impact': {}\n",
    "    }\n",
    "\n",
    "    feature_impact = pd.DataFrame({\n",
    "        'feature': X_test.columns,\n",
    "        'shap_value': single_prediction_shap_values[0]\n",
    "    }).sort_values(by='shap_value', key=abs, ascending=False)\n",
    "\n",
    "    for _, row in feature_impact.head(num_top_features).iterrows():\n",
    "        single_shap_dict['top_features_impact'][row['feature']] = float(row['shap_value'])\n",
    "\n",
    "    prediction_outcome = \"Default\" if single_shap_dict['predicted_proba'] >= optimized_threshold else \"No Default\"\n",
    "    actual_outcome = \"Default\" if single_shap_dict['actual_label'] == 1 else \"No Default\"\n",
    "\n",
    "    print(f\"\\n### Selected Borrower SHAP Insights (Random Sample) ###\")\n",
    "    print(f\"- Loan Index: {single_shap_dict['index']}\")\n",
    "    print(f\"- Predicted Outcome: {prediction_outcome} (Probability: {single_shap_dict['predicted_proba']:.4f})\")\n",
    "    print(f\"- Actual Outcome: {actual_outcome}\")\n",
    "    print(f\"- Top {num_top_features} Contributing Factors:\")\n",
    "    for feat, val in single_shap_dict['top_features_impact'].items():\n",
    "        print(f\"-> {feat}: {val:.4f}\")\n",
    "\n",
    "    return single_shap_dict\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    print(\"### Starting XGBoost Base Model Training and Evaluation Workflow ###\")\n",
    "\n",
    "    X_train, y_train, X_test, y_test = load_data()\n",
    "\n",
    "    model = train_model(X_train, y_train)\n",
    "    print(\"XGBoost model training complete.\")\n",
    "\n",
    "    joblib.dump(model, MODEL_PATH)\n",
    "    print(f\"Model saved to {MODEL_PATH}\")\n",
    "\n",
    "    print(\"\\n### Starting Model Evaluation and Analysis ###\")\n",
    "\n",
    "    best_threshold, _ = optimize_f1_threshold(model, X_test, y_test, EVAL_SAVE_DIR)\n",
    "    print(f\"F1-score optimization complete. Best Threshold: {best_threshold:.4f}\")\n",
    "\n",
    "    evaluate_model_metrics_and_plots(model, X_test, y_test, threshold=best_threshold, eval_save_dir=EVAL_SAVE_DIR)\n",
    "    print(\"Core model evaluation complete and plots saved.\")\n",
    "\n",
    "    bootstrap_evaluation(model, X_test, y_test)\n",
    "    print(\"Bootstrap evaluation complete.\")\n",
    "\n",
    "    shap_analysis_global(model, X_train, EVAL_SAVE_DIR)\n",
    "    print(\"Global SHAP plots generated and saved.\")\n",
    "\n",
    "    single_borrower_insights = export_shap_insights_for_single_borrower(model, X_test, y_test, best_threshold)\n",
    "    print(\"SHAP insights for a single borrower exported.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (shap-env)",
   "language": "python",
   "name": "shap-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
