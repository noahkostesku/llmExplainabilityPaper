{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e1e30da9-45ad-4f5a-8f2b-44053e622e48",
   "metadata": {},
   "source": [
    "## GAT-Based Loan Default Prediction\n",
    "\n",
    "This notebook implements a GAT to predict loan defaults on graph-structured data. It includes custom metrics, focal loss for class imbalance, and NeighborLoader sampling for efficient mini-batch training. Model performance is evaluated using AUC, F1-score, and bootstrap confidence intervals.\n",
    "\n",
    "Note: This script is intended for academic reference only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c20bde71-6d5f-4570-b1d6-5c6c8271b172",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GATConv, TransformerConv\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.loader import NeighborLoader\n",
    "\n",
    "import os\n",
    "\n",
    "def find_best_f1_threshold(labels, probs):\n",
    "    \n",
    "    best_f1, best_threshold = 0.0, 0.5\n",
    "    for t in np.linspace(0.1, 0.9, 81):\n",
    "        preds = (probs > t).astype(int)\n",
    "        f1 = custom_f1_score(labels, preds)\n",
    "        if f1 > best_f1:\n",
    "            best_f1 = f1\n",
    "            best_threshold = t\n",
    "    return best_threshold, best_f1\n",
    "\n",
    "def custom_f1_score(y_true, y_pred_binary):\n",
    "    \n",
    "    y_true = np.asarray(y_true)\n",
    "    y_pred_binary = np.asarray(y_pred_binary)\n",
    "    y_true_bool = y_true.astype(bool)\n",
    "    y_pred_bool = y_pred_binary.astype(bool)\n",
    "    tp = np.sum(y_true_bool & y_pred_bool)\n",
    "    fp = np.sum(~y_true_bool & y_pred_bool)\n",
    "    fn = np.sum(y_true_bool & ~y_pred_bool)\n",
    "    precision = tp / (tp + fp) if (tp + fp) > 0 else 0.0\n",
    "    recall = tp / (tp + fn) if (tp + fn) > 0 else 0.0\n",
    "    f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0.0\n",
    "    \n",
    "    return f1\n",
    "\n",
    "def custom_roc_auc_score(y_true, y_scores):\n",
    "    \n",
    "    y_true = np.asarray(y_true)\n",
    "    y_scores = np.asarray(y_scores)\n",
    "    sorted_indices = np.argsort(y_scores)[::-1]\n",
    "    y_true_sorted = y_true[sorted_indices]\n",
    "    num_positive = np.sum(y_true == 1)\n",
    "    num_negative = np.sum(y_true == 0)\n",
    "    tpr_points = [0.0] \n",
    "    fpr_points = [0.0]\n",
    "    tp_count = 0\n",
    "    fp_count = 0\n",
    "\n",
    "    for i in range(len(y_true_sorted)):\n",
    "        \n",
    "        if y_true_sorted[i] == 1:\n",
    "            tp_count += 1\n",
    "        else:\n",
    "            fp_count += 1\n",
    "            \n",
    "        current_tpr = tp_count / num_positive\n",
    "        current_fpr = fp_count / num_negative\n",
    "\n",
    "        if not (current_tpr == tpr_points[-1] and current_fpr == fpr_points[-1]):\n",
    "            tpr_points.append(current_tpr)\n",
    "            fpr_points.append(current_fpr)\n",
    "\n",
    "    if fpr_points[-1] != 1.0 or tpr_points[-1] != 1.0:\n",
    "        fpr_points.append(1.0)\n",
    "        tpr_points.append(1.0)\n",
    "\n",
    "    auc = 0.0\n",
    "    for i in range(len(fpr_points) - 1):\n",
    "        auc += (fpr_points[i+1] - fpr_points[i]) * (tpr_points[i+1] + tpr_points[i]) / 2.0\n",
    "            \n",
    "    return auc\n",
    "\n",
    "def custom_resample(data_array, replace=True, n_samples=None, random_state=None):\n",
    "\n",
    "    if n_samples is None:\n",
    "        n_samples = len(data_array)\n",
    "            \n",
    "    if random_state is not None:\n",
    "        np.random.seed(random_state)\n",
    "            \n",
    "    return np.random.choice(data_array, size=n_samples, replace=replace)\n",
    "\n",
    "class FocalLoss(nn.Module):\n",
    "    \n",
    "    def __init__(self, alpha=1, gamma=2, logits=True, reduce=True):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.logits = logits\n",
    "        self.reduce = reduce\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        if self.logits:\n",
    "            BCE_loss = F.binary_cross_entropy_with_logits(inputs, targets, reduction='none')\n",
    "        else:\n",
    "            BCE_loss = F.binary_cross_entropy(inputs, targets, reduction='none')\n",
    "        pt = torch.exp(-BCE_loss)\n",
    "        F_loss = self.alpha * (1 - pt) ** self.gamma * BCE_loss\n",
    "\n",
    "        return F_loss.mean() if self.reduce else F_loss\n",
    "\n",
    "class Config:\n",
    "\n",
    "    TRAIN_GRAPH_PATH = \"../data/graph_data/train_graph.pt\"\n",
    "    TEST_GRAPH_PATH = \"../data/graph_data/test_graph.pt\"\n",
    "    MODEL_SAVE_PATH = \"./trained_gat_model.pt\"\n",
    "    IN_CHANNELS = 13\n",
    "    HIDDEN_CHANNELS = 128\n",
    "    OUT_CHANNELS = 1\n",
    "    HEADS = 8 \n",
    "    NUM_NEIGHBORS = [20, 10]\n",
    "    LEARNING_RATE = 0.005\n",
    "    EPOCHS = 200\n",
    "    BATCH_SIZE = 128\n",
    "    NUM_WORKERS = 4\n",
    "    EDGE_DIM = 4 \n",
    "\n",
    "class GAT(nn.Module):\n",
    "    \n",
    "    def __init__(self, in_channels, hidden_channels, out_channels, heads,\n",
    "                 attn_dropout=0.6, feat_dropout=0.5, edge_dim=None):\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv1 = TransformerConv(in_channels, hidden_channels,\n",
    "                                     heads=heads, concat=True,\n",
    "                                     dropout=attn_dropout,\n",
    "                                     edge_dim=edge_dim)\n",
    "        self.norm1 = nn.LayerNorm(hidden_channels * heads)\n",
    "        self.dropout = nn.Dropout(feat_dropout)\n",
    "        self.conv2 = TransformerConv(hidden_channels * heads, out_channels,\n",
    "                                     heads=1, concat=False,\n",
    "                                     dropout=attn_dropout,\n",
    "                                     edge_dim=edge_dim)\n",
    "\n",
    "    def forward(self, x, edge_index, edge_attr):\n",
    "        x = self.conv1(x, edge_index, edge_attr)\n",
    "        x = self.norm1(x)              \n",
    "        x = F.elu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.conv2(x, edge_index, edge_attr)\n",
    "        return x\n",
    "\n",
    "def load_graph_data(path):\n",
    "    \n",
    "    data = torch.load(path)\n",
    "    if hasattr(data, 'edge_index') and data.edge_index is not None:\n",
    "        if data.edge_index.dtype != torch.long:\n",
    "            print(f\"Warning: Converting edge_index from {data.edge_index.dtype} to torch.long for {path}\")\n",
    "            data.edge_index = data.edge_index.long()\n",
    "        if not data.edge_index.is_contiguous():\n",
    "            print(f\"Warning: Making edge_index contiguous for {path}\")\n",
    "            data.edge_index = data.edge_index.contiguous()\n",
    "            \n",
    "    if hasattr(data, 'edge_attr') and data.edge_attr is not None:\n",
    "        if data.edge_attr.dtype != torch.float:\n",
    "            print(f\"Warning: Converting edge_attr from {data.edge_attr.dtype} to torch.float for {path}\")\n",
    "            data.edge_attr = data.edge_attr.float()\n",
    "            \n",
    "    if hasattr(data, 'default') and data.default is not None:\n",
    "        labels = data.default\n",
    "        if labels.dtype != torch.float:\n",
    "            print(f\"Warning: Converting 'default' labels from {labels.dtype} to torch.float for {path}\")\n",
    "            data.y = labels.float()\n",
    "        else:\n",
    "            data.y = labels\n",
    "        del data.default\n",
    "    elif hasattr(data, 'y') and data.y is not None:\n",
    "        if data.y.dtype != torch.float:\n",
    "            print(f\"Warning: Converting existing 'y' labels from {data.y.dtype} to torch.float for {path}\")\n",
    "            data.y = data.y.float()\n",
    "    else:\n",
    "        print(f\"Warning: No 'y' or 'default' labels found in {path}. Please ensure labels are correctly loaded.\")\n",
    "\n",
    "    return data\n",
    "\n",
    "def calculate_accuracy(predictions, targets):\n",
    "    correct = (predictions == targets).sum().item()\n",
    "    acc = correct / targets.size(0)\n",
    "    return acc\n",
    "\n",
    "def train():\n",
    "    full_train_data = load_graph_data(Config.TRAIN_GRAPH_PATH)\n",
    "    full_test_data = load_graph_data(Config.TEST_GRAPH_PATH)\n",
    "\n",
    "    full_train_data = full_train_data.to('cpu')\n",
    "    full_test_data = full_test_data.to('cpu')\n",
    "\n",
    "    print(\"\\n--- Loaded Graph Data Memory Footprint (Full Graph) ---\")\n",
    "    total_loaded_mem_bytes = 0\n",
    "    if hasattr(full_train_data, 'x') and full_train_data.x is not None:\n",
    "        print(f\"Train features (x) shape: {full_train_data.x.shape}\")\n",
    "        x_mem_bytes = full_train_data.x.element_size() * full_train_data.x.nelement()\n",
    "        total_loaded_mem_bytes += x_mem_bytes\n",
    "        print(f\"Train features (x) size: {x_mem_bytes / (1024**3):.2f} GB (dtype: {full_train_data.x.dtype})\")\n",
    "    if hasattr(full_train_data, 'edge_index') and full_train_data.edge_index is not None:\n",
    "        print(f\"Train edge_index shape: {full_train_data.edge_index.shape}\")\n",
    "        edge_index_mem_bytes = full_train_data.edge_index.element_size() * full_train_data.edge_index.nelement()\n",
    "        total_loaded_mem_bytes += edge_index_mem_bytes\n",
    "        print(f\"Train edge_index size: {edge_index_mem_bytes / (1024**3):.2f} GB (dtype: {full_train_data.edge_index.dtype})\")\n",
    "    if hasattr(full_train_data, 'edge_attr') and full_train_data.edge_attr is not None:\n",
    "        print(f\"Train edge_attr shape: {full_train_data.edge_attr.shape}\")\n",
    "        edge_attr_mem_bytes = full_train_data.edge_attr.element_size() * full_train_data.edge_attr.nelement()\n",
    "        total_loaded_mem_bytes += edge_attr_mem_bytes\n",
    "        print(f\"Train edge_attr size: {edge_attr_mem_bytes / (1024**3):.2f} GB (dtype: {full_train_data.edge_attr.dtype})\")\n",
    "    if hasattr(full_train_data, 'y') and full_train_data.y is not None:\n",
    "        print(f\"Train labels (y) shape: {full_train_data.y.shape}\")\n",
    "        y_mem_bytes = full_train_data.y.element_size() * full_train_data.y.nelement()\n",
    "        total_loaded_mem_bytes += y_mem_bytes\n",
    "        print(f\"Train labels (y) size: {y_mem_bytes / (1024**3):.2f} GB (dtype: {full_train_data.y.dtype})\")\n",
    "            \n",
    "    print(f\"Total estimated memory for full_train_data: {total_loaded_mem_bytes / (1024**3):.2f} GB\\n\")\n",
    "\n",
    "    in_channels = full_train_data.x.shape[1]\n",
    "    out_channels = Config.OUT_CHANNELS\n",
    "\n",
    "    model = GAT(in_channels=in_channels,\n",
    "                hidden_channels=Config.HIDDEN_CHANNELS,\n",
    "                out_channels=out_channels,\n",
    "                heads=Config.HEADS,\n",
    "                edge_dim=Config.EDGE_DIM)\n",
    "    model.to('cpu')\n",
    "            \n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=Config.LEARNING_RATE, weight_decay=5e-4)\n",
    "    num_positive_train = torch.sum(full_train_data.y == 1).item()\n",
    "    num_negative_train = torch.sum(full_train_data.y == 0).item()\n",
    "    \n",
    "    if num_positive_train == 0:\n",
    "        print(\"Warning: No positive samples in training data.\")\n",
    "        pos_weight = torch.tensor([1.0], dtype=torch.float32)\n",
    "    else:\n",
    "        pos_weight_value = num_negative_train / num_positive_train\n",
    "        pos_weight = torch.tensor([pos_weight_value], dtype=torch.float32).to('cpu')\n",
    "    \n",
    "    criterion = FocalLoss(alpha=1, gamma=2, logits=True)\n",
    "    positives = torch.where(full_train_data.y == 1)[0]\n",
    "    negatives = torch.where(full_train_data.y == 0)[0]\n",
    " \n",
    "    target_neg_samples = len(positives) * 5\n",
    "    if target_neg_samples > len(negatives):\n",
    "        print(f\"Warning: Desired negative samples ({target_neg_samples}) exceeds available negatives ({len(negatives)}). Sampling all negatives.\")\n",
    "        sampled_negatives = negatives\n",
    "    else:\n",
    "        sampled_negatives = negatives[torch.randperm(len(negatives))[:target_neg_samples]]\n",
    "    \n",
    "    balanced_nodes = torch.cat([positives, sampled_negatives])\n",
    "    print(f\"Training with oversampled nodes: {len(positives)} positives, {len(sampled_negatives)} sampled negatives.\")\n",
    "    \n",
    "    train_loader = NeighborLoader(\n",
    "        full_train_data,\n",
    "        num_neighbors=Config.NUM_NEIGHBORS,\n",
    "        batch_size=Config.BATCH_SIZE,\n",
    "        input_nodes=balanced_nodes,\n",
    "        shuffle=True,\n",
    "        num_workers=Config.NUM_WORKERS,\n",
    "        drop_last=True\n",
    "    )\n",
    "\n",
    "    test_loader = NeighborLoader(\n",
    "        full_test_data,\n",
    "        num_neighbors=Config.NUM_NEIGHBORS,\n",
    "        batch_size=Config.BATCH_SIZE,\n",
    "        input_nodes=torch.arange(full_test_data.num_nodes),\n",
    "        shuffle=False,\n",
    "        num_workers=Config.NUM_WORKERS\n",
    "    )\n",
    "\n",
    "    best_auc = 0.0\n",
    "    counter = 0\n",
    "    patience = 10\n",
    "    best_model_state = None\n",
    "\n",
    "    for epoch in range(Config.EPOCHS):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        for batch_idx, batch_data in enumerate(train_loader):\n",
    "            batch_x = batch_data.x.to('cpu')\n",
    "            batch_edge_index = batch_data.edge_index.to('cpu')\n",
    "            batch_labels = batch_data.y.to('cpu')\n",
    "            batch_edge_attr = batch_data.edge_attr.to('cpu')\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            out_full_subgraph = model(batch_x, batch_edge_index, batch_edge_attr).squeeze()\n",
    "            bs = batch_data.batch_size if hasattr(batch_data, 'batch_size') else Config.BATCH_SIZE\n",
    "            out = out_full_subgraph[:bs]\n",
    "            labels = batch_labels[:bs]\n",
    "\n",
    "            loss = criterion(out, labels.float())\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "\n",
    "        print(f'Epoch {epoch:03d} — Avg Loss: {total_loss / len(train_loader):.4f}')\n",
    "\n",
    "        model.eval()\n",
    "        all_preds_proba = []\n",
    "        all_labels = []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for batch_idx, batch_data in enumerate(test_loader):\n",
    "                batch_x = batch_data.x.to('cpu')\n",
    "                batch_edge_index = batch_data.edge_index.to('cpu')\n",
    "                batch_labels = batch_data.y[:batch_data.batch_size].to('cpu').long()\n",
    "                batch_edge_attr = batch_data.edge_attr.to('cpu')\n",
    "                outputs_full = model(batch_x, batch_edge_index, batch_edge_attr).squeeze()\n",
    "                outputs = outputs_full[:batch_data.batch_size]\n",
    "                probs = torch.sigmoid(outputs)\n",
    "                all_preds_proba.extend(probs.tolist())\n",
    "                all_labels.extend(batch_labels.tolist())\n",
    "\n",
    "        final_preds_proba = np.array(all_preds_proba)\n",
    "        final_labels = np.array(all_labels)\n",
    "        print(\"Predicted probabilities range:\", np.min(final_preds_proba), \"-\", np.max(final_preds_proba))\n",
    "\n",
    "        if np.max(final_preds_proba) < 0.1:\n",
    "            print(\"Warning: Narrow prediction range. Model may not be learning effectively.\")\n",
    "\n",
    "        threshold, best_f1 = find_best_f1_threshold(final_labels, final_preds_proba)\n",
    "        final_preds_binary = (final_preds_proba > threshold).astype(int)\n",
    "        print(f\"Optimal threshold based on F1: {threshold:.2f}\")\n",
    "\n",
    "        try:\n",
    "            auc_score = custom_roc_auc_score(final_labels, final_preds_proba)\n",
    "            f1 = custom_f1_score(final_labels, final_preds_binary)\n",
    "            acc = calculate_accuracy(torch.tensor(final_preds_binary), torch.tensor(final_labels))\n",
    "\n",
    "            print(f\"AUC: {auc_score:.4f} — F1: {f1:.4f} — Acc: {acc:.4f}\")\n",
    "        except ValueError as e:\n",
    "            print(f\"Metric computation error: {e}\")\n",
    "            auc_score = 0\n",
    "\n",
    "        if auc_score > best_auc:\n",
    "            best_auc = auc_score\n",
    "            best_model_state = model.state_dict()\n",
    "            counter = 0\n",
    "            print(f\"NEW BEST AUC: {auc_score:.4f}\")\n",
    "        else:\n",
    "            counter += 1\n",
    "            print(f\"No improvement for {counter} epoch(s).\")\n",
    "            if counter >= patience:\n",
    "                print(\"### Early stopping triggered ###\")\n",
    "                break\n",
    "\n",
    "    if best_model_state is not None:\n",
    "        model.load_state_dict(best_model_state)\n",
    "        print(\"### Restored best model weights (early stopping) ###\")\n",
    "\n",
    "    n_bootstraps = 1000\n",
    "    bootstrapped_auc_scores = []\n",
    "    bootstrapped_f1_scores = []\n",
    "\n",
    "    print(f\"\\nPerforming {n_bootstraps} bootstraps for AUC and F1-Score.\")\n",
    "\n",
    "    for i in range(n_bootstraps):\n",
    "        indices = custom_resample(np.arange(len(final_labels)), replace=True, n_samples=len(final_labels), random_state=i)\n",
    "\n",
    "        if len(np.unique(final_labels[indices])) < 2:\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            auc_bootstrap = custom_roc_auc_score(final_labels[indices], final_preds_proba[indices])\n",
    "            bootstrapped_auc_scores.append(auc_bootstrap)\n",
    "\n",
    "            f1_bootstrap = custom_f1_score(\n",
    "                final_labels[indices],\n",
    "                (final_preds_proba[indices] > threshold).astype(int)\n",
    "            )\n",
    "            bootstrapped_f1_scores.append(f1_bootstrap)\n",
    "        except ValueError:\n",
    "            continue\n",
    "\n",
    "    if bootstrapped_auc_scores:\n",
    "        print(f\"Bootstrapped AUC (Mean): {np.mean(bootstrapped_auc_scores):.4f}\")\n",
    "        print(f\"Bootstrapped AUC (95% CI): ({np.percentile(bootstrapped_auc_scores, 2.5):.4f}, {np.percentile(bootstrapped_auc_scores, 97.5):.4f})\")\n",
    "    else:\n",
    "        print(\"Not enough valid bootstrap samples to calculate AUC statistics.\")\n",
    "\n",
    "    if bootstrapped_f1_scores:\n",
    "        print(f\"Bootstrapped F1-Score (Mean): {np.mean(bootstrapped_f1_scores):.4f}\")\n",
    "        print(f\"Bootstrapped F1-Score (95% CI): ({np.percentile(bootstrapped_f1_scores, 2.5):.4f}, {np.percentile(bootstrapped_f1_scores, 97.5):.4f})\")\n",
    "    else:\n",
    "        print(\"Not enough valid bootstrap samples to calculate F1-Score statistics.\")\n",
    "\n",
    "    os.makedirs(os.path.dirname(Config.MODEL_SAVE_PATH), exist_ok=True)\n",
    "    torch.save(model.state_dict(), Config.MODEL_SAVE_PATH)\n",
    "    print(f\"\\nModel saved to: {Config.MODEL_SAVE_PATH}\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    if not os.path.exists(os.path.dirname(Config.TRAIN_GRAPH_PATH)):\n",
    "        print(f\"Error: Data directory '{os.path.dirname(Config.TRAIN_GRAPH_PATH)}' not found.\")\n",
    "        exit()\n",
    "\n",
    "    train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
